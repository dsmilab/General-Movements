{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Load\" data-toc-modified-id=\"Load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load</a></span></li><li><span><a href=\"#LGBM-training\" data-toc-modified-id=\"LGBM-training-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LGBM training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare\" data-toc-modified-id=\"Prepare-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Prepare</a></span></li><li><span><a href=\"#Core\" data-toc-modified-id=\"Core-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Core</a></span></li><li><span><a href=\"#Feature-importances\" data-toc-modified-id=\"Feature-importances-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Feature importances</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:19:41.012217Z",
     "start_time": "2018-07-26T07:19:34.430781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from apps.lib import Sensor, Performance, Model\n",
    "from apps.score import ScoreBoard\n",
    "import apps.tkconfig as tkconfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "SONG = 1\n",
    "WHO = 7\n",
    "TRAIN_ORDER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:19:42.976057Z",
     "start_time": "2018-07-26T07:19:41.014519Z"
    }
   },
   "outputs": [],
   "source": [
    "sensor_data = Sensor(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:19:44.565545Z",
     "start_time": "2018-07-26T07:19:42.977409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/bin/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/bin/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/bin/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "scoreboard = ScoreBoard(sensor_data.drummer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:19:44.569669Z",
     "start_time": "2018-07-26T07:19:44.566969Z"
    }
   },
   "outputs": [],
   "source": [
    "def gogo(who_id, song_id, train_pf_order):\n",
    "    train_pf = Performance(sensor_data, who_id, song_id, train_pf_order, 4, 2)\n",
    "    \n",
    "    pfs = []\n",
    "    for pf_order in tqdm(range(1, 3 + 1)):\n",
    "        if pf_order == train_pf_order:\n",
    "            pfs.append(train_pf)\n",
    "        else:\n",
    "            pfs.append(Performance(sensor_data, who_id, song_id, pf_order, 4, 2)) \n",
    "            \n",
    "    return pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:26:40.622139Z",
     "start_time": "2018-07-26T07:19:44.570866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:34<00:00, 11.65s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.48s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.44s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.50s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.62s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.55s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.59s/it]\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.65s/it]\n"
     ]
    }
   ],
   "source": [
    "pfs = []\n",
    "for drumer in range(1, 8 + 1):\n",
    "    ppf = gogo(drumer, SONG, TRAIN_ORDER)\n",
    "    pfs.append(ppf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:26:40.626207Z",
     "start_time": "2018-07-26T07:26:40.623670Z"
    }
   },
   "outputs": [],
   "source": [
    "def soso(who_id, song_id):\n",
    "    sfs = []\n",
    "    for pf_order in range(1, 3 + 1):\n",
    "        img_scores, _ = scoreboard.get_scores_with_timestamps(who_id, song_id, pf_order)\n",
    "        sfs.append(img_scores[-1])\n",
    "    return sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:28:26.275857Z",
     "start_time": "2018-07-26T07:26:40.629335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3984/3984 [==============================] - 0s     \n",
      "3808/3900 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "sfs = []\n",
    "for drumer in range(1, 8 + 1):\n",
    "    sfs.append(soso(drumer, SONG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:28:26.332155Z",
     "start_time": "2018-07-26T07:28:26.277569Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_type</th>\n",
       "      <th>L_AAI</th>\n",
       "      <th>L_AVI</th>\n",
       "      <th>L_ASMA</th>\n",
       "      <th>L_GAI</th>\n",
       "      <th>L_GVI</th>\n",
       "      <th>L_GSMA</th>\n",
       "      <th>L_AAE</th>\n",
       "      <th>L_ARE</th>\n",
       "      <th>L_MAMI</th>\n",
       "      <th>...</th>\n",
       "      <th>R_AZCR</th>\n",
       "      <th>R_GZCR</th>\n",
       "      <th>R_AMCR</th>\n",
       "      <th>R_GMCR</th>\n",
       "      <th>R_AXYCORR</th>\n",
       "      <th>R_AYZCORR</th>\n",
       "      <th>R_AZXCORR</th>\n",
       "      <th>R_GXYCORR</th>\n",
       "      <th>R_GYZCORR</th>\n",
       "      <th>R_GZXCORR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.144363</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.182714</td>\n",
       "      <td>7.837218</td>\n",
       "      <td>16.855832</td>\n",
       "      <td>11.142000</td>\n",
       "      <td>0.021825</td>\n",
       "      <td>78.277812</td>\n",
       "      <td>0.126178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.544855</td>\n",
       "      <td>-0.684912</td>\n",
       "      <td>0.121344</td>\n",
       "      <td>0.752108</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>-0.055694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.172753</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>31.572826</td>\n",
       "      <td>161.915005</td>\n",
       "      <td>47.817571</td>\n",
       "      <td>0.035236</td>\n",
       "      <td>1158.758338</td>\n",
       "      <td>0.183308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.185425</td>\n",
       "      <td>-0.418489</td>\n",
       "      <td>0.697969</td>\n",
       "      <td>-0.488330</td>\n",
       "      <td>0.925166</td>\n",
       "      <td>-0.747645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.152061</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.226429</td>\n",
       "      <td>62.496385</td>\n",
       "      <td>28.197039</td>\n",
       "      <td>79.462143</td>\n",
       "      <td>0.023488</td>\n",
       "      <td>3933.995127</td>\n",
       "      <td>0.156170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.786548</td>\n",
       "      <td>0.911355</td>\n",
       "      <td>-0.795170</td>\n",
       "      <td>0.975555</td>\n",
       "      <td>-0.810204</td>\n",
       "      <td>-0.866550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.431467</td>\n",
       "      <td>0.090386</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>56.532036</td>\n",
       "      <td>555.794405</td>\n",
       "      <td>83.879000</td>\n",
       "      <td>0.276550</td>\n",
       "      <td>3751.665457</td>\n",
       "      <td>0.233283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.835047</td>\n",
       "      <td>-0.485918</td>\n",
       "      <td>0.075094</td>\n",
       "      <td>-0.244019</td>\n",
       "      <td>0.141725</td>\n",
       "      <td>-0.994565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.390009</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>0.629600</td>\n",
       "      <td>40.664224</td>\n",
       "      <td>350.779650</td>\n",
       "      <td>63.690200</td>\n",
       "      <td>0.167024</td>\n",
       "      <td>2004.358734</td>\n",
       "      <td>0.346880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>-0.060541</td>\n",
       "      <td>-0.043862</td>\n",
       "      <td>-0.886426</td>\n",
       "      <td>0.493714</td>\n",
       "      <td>-0.532830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.201691</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>42.791711</td>\n",
       "      <td>18.822723</td>\n",
       "      <td>72.489400</td>\n",
       "      <td>0.043866</td>\n",
       "      <td>1849.953245</td>\n",
       "      <td>0.185637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.487971</td>\n",
       "      <td>0.100405</td>\n",
       "      <td>-0.038495</td>\n",
       "      <td>0.689536</td>\n",
       "      <td>-0.754955</td>\n",
       "      <td>-0.864607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.127174</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>14.423266</td>\n",
       "      <td>37.821719</td>\n",
       "      <td>23.181800</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>245.852329</td>\n",
       "      <td>0.124161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.660409</td>\n",
       "      <td>0.870875</td>\n",
       "      <td>-0.813031</td>\n",
       "      <td>-0.115209</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>0.833729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>11.464270</td>\n",
       "      <td>6.423106</td>\n",
       "      <td>17.134200</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>137.852601</td>\n",
       "      <td>0.087926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.865026</td>\n",
       "      <td>-0.689175</td>\n",
       "      <td>0.676779</td>\n",
       "      <td>-0.055290</td>\n",
       "      <td>0.310975</td>\n",
       "      <td>0.007419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.152996</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>18.565111</td>\n",
       "      <td>3.307427</td>\n",
       "      <td>27.880600</td>\n",
       "      <td>0.023992</td>\n",
       "      <td>347.970792</td>\n",
       "      <td>0.149114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.669668</td>\n",
       "      <td>-0.409309</td>\n",
       "      <td>0.522083</td>\n",
       "      <td>-0.256624</td>\n",
       "      <td>-0.473077</td>\n",
       "      <td>0.119202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.328453</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>53.995201</td>\n",
       "      <td>1544.527742</td>\n",
       "      <td>80.151000</td>\n",
       "      <td>0.125935</td>\n",
       "      <td>4460.009459</td>\n",
       "      <td>0.299490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.170319</td>\n",
       "      <td>0.921414</td>\n",
       "      <td>0.224085</td>\n",
       "      <td>-0.725166</td>\n",
       "      <td>0.809888</td>\n",
       "      <td>-0.624116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.309602</td>\n",
       "      <td>0.028823</td>\n",
       "      <td>0.474667</td>\n",
       "      <td>41.886609</td>\n",
       "      <td>358.011245</td>\n",
       "      <td>68.174000</td>\n",
       "      <td>0.124677</td>\n",
       "      <td>2112.499227</td>\n",
       "      <td>0.231707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.409492</td>\n",
       "      <td>-0.904054</td>\n",
       "      <td>-0.127712</td>\n",
       "      <td>0.795752</td>\n",
       "      <td>-0.663151</td>\n",
       "      <td>-0.788868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.194255</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>18.869593</td>\n",
       "      <td>15.297130</td>\n",
       "      <td>29.930250</td>\n",
       "      <td>0.037857</td>\n",
       "      <td>371.358653</td>\n",
       "      <td>0.197707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.073651</td>\n",
       "      <td>-0.706916</td>\n",
       "      <td>-0.474636</td>\n",
       "      <td>0.384338</td>\n",
       "      <td>-0.732997</td>\n",
       "      <td>-0.313854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.134342</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.187333</td>\n",
       "      <td>9.563026</td>\n",
       "      <td>7.483308</td>\n",
       "      <td>14.086833</td>\n",
       "      <td>0.018320</td>\n",
       "      <td>98.934765</td>\n",
       "      <td>0.135733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.235666</td>\n",
       "      <td>0.333801</td>\n",
       "      <td>0.437924</td>\n",
       "      <td>-0.058161</td>\n",
       "      <td>-0.157697</td>\n",
       "      <td>0.574602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.098452</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>12.379005</td>\n",
       "      <td>26.802259</td>\n",
       "      <td>19.109800</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>180.042036</td>\n",
       "      <td>0.099479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.214161</td>\n",
       "      <td>0.489343</td>\n",
       "      <td>-0.631098</td>\n",
       "      <td>-0.402869</td>\n",
       "      <td>0.334590</td>\n",
       "      <td>0.099094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.084664</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>5.559359</td>\n",
       "      <td>5.815340</td>\n",
       "      <td>8.936250</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>36.721813</td>\n",
       "      <td>0.082901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.442770</td>\n",
       "      <td>-0.594044</td>\n",
       "      <td>0.275310</td>\n",
       "      <td>0.977695</td>\n",
       "      <td>-0.953086</td>\n",
       "      <td>-0.992685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.077464</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.093250</td>\n",
       "      <td>8.528079</td>\n",
       "      <td>6.506929</td>\n",
       "      <td>12.662500</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>79.235062</td>\n",
       "      <td>0.075244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.908620</td>\n",
       "      <td>-0.331470</td>\n",
       "      <td>-0.092264</td>\n",
       "      <td>0.117552</td>\n",
       "      <td>-0.214236</td>\n",
       "      <td>-0.949340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.097237</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.139571</td>\n",
       "      <td>13.555943</td>\n",
       "      <td>25.983775</td>\n",
       "      <td>20.488857</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>209.747375</td>\n",
       "      <td>0.104484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.231539</td>\n",
       "      <td>-0.017266</td>\n",
       "      <td>0.135815</td>\n",
       "      <td>0.285040</td>\n",
       "      <td>-0.563274</td>\n",
       "      <td>0.278315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.102223</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.142429</td>\n",
       "      <td>11.472766</td>\n",
       "      <td>13.361363</td>\n",
       "      <td>17.047286</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>144.985724</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.449289</td>\n",
       "      <td>0.764545</td>\n",
       "      <td>-0.831028</td>\n",
       "      <td>-0.974738</td>\n",
       "      <td>0.925727</td>\n",
       "      <td>-0.903211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.088157</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.116714</td>\n",
       "      <td>16.533870</td>\n",
       "      <td>15.443233</td>\n",
       "      <td>24.418571</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>288.812074</td>\n",
       "      <td>0.122057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.476185</td>\n",
       "      <td>0.669930</td>\n",
       "      <td>0.311848</td>\n",
       "      <td>0.852547</td>\n",
       "      <td>-0.590252</td>\n",
       "      <td>-0.803097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.360621</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>84.131220</td>\n",
       "      <td>1144.326018</td>\n",
       "      <td>125.163250</td>\n",
       "      <td>0.149899</td>\n",
       "      <td>8222.388277</td>\n",
       "      <td>0.378034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.524693</td>\n",
       "      <td>0.597842</td>\n",
       "      <td>-0.147682</td>\n",
       "      <td>0.268676</td>\n",
       "      <td>0.534829</td>\n",
       "      <td>-0.639858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.293364</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>62.188938</td>\n",
       "      <td>85.000956</td>\n",
       "      <td>98.146600</td>\n",
       "      <td>0.087387</td>\n",
       "      <td>3952.464974</td>\n",
       "      <td>0.311443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.300233</td>\n",
       "      <td>-0.307793</td>\n",
       "      <td>-0.799677</td>\n",
       "      <td>-0.139533</td>\n",
       "      <td>0.461526</td>\n",
       "      <td>-0.638487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.270309</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.425750</td>\n",
       "      <td>33.561496</td>\n",
       "      <td>383.309644</td>\n",
       "      <td>55.995750</td>\n",
       "      <td>0.075316</td>\n",
       "      <td>1509.683663</td>\n",
       "      <td>0.271377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.546529</td>\n",
       "      <td>-0.325672</td>\n",
       "      <td>-0.210766</td>\n",
       "      <td>0.522460</td>\n",
       "      <td>0.729067</td>\n",
       "      <td>0.732887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.163944</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.227167</td>\n",
       "      <td>9.197816</td>\n",
       "      <td>7.981695</td>\n",
       "      <td>13.277167</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>92.581520</td>\n",
       "      <td>0.162121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.329155</td>\n",
       "      <td>0.202903</td>\n",
       "      <td>-0.893932</td>\n",
       "      <td>0.191525</td>\n",
       "      <td>-0.917464</td>\n",
       "      <td>-0.489747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.260209</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>19.859446</td>\n",
       "      <td>468.626553</td>\n",
       "      <td>30.719750</td>\n",
       "      <td>0.071294</td>\n",
       "      <td>863.024136</td>\n",
       "      <td>0.251451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.537622</td>\n",
       "      <td>0.132907</td>\n",
       "      <td>-0.672478</td>\n",
       "      <td>-0.289827</td>\n",
       "      <td>-0.417150</td>\n",
       "      <td>0.372505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>0.564537</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.815400</td>\n",
       "      <td>100.073674</td>\n",
       "      <td>786.448232</td>\n",
       "      <td>151.047600</td>\n",
       "      <td>0.328111</td>\n",
       "      <td>10801.188459</td>\n",
       "      <td>0.583504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.631941</td>\n",
       "      <td>-0.125805</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.319282</td>\n",
       "      <td>0.519681</td>\n",
       "      <td>0.243485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>1.136833</td>\n",
       "      <td>86.229255</td>\n",
       "      <td>436.178918</td>\n",
       "      <td>129.587000</td>\n",
       "      <td>0.603048</td>\n",
       "      <td>7871.663289</td>\n",
       "      <td>0.808732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.425767</td>\n",
       "      <td>-0.708604</td>\n",
       "      <td>0.605424</td>\n",
       "      <td>-0.925964</td>\n",
       "      <td>0.772844</td>\n",
       "      <td>-0.813119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>0.657002</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>157.180415</td>\n",
       "      <td>4321.316139</td>\n",
       "      <td>244.170750</td>\n",
       "      <td>0.442607</td>\n",
       "      <td>29026.998923</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.556245</td>\n",
       "      <td>-0.258675</td>\n",
       "      <td>0.198119</td>\n",
       "      <td>-0.672692</td>\n",
       "      <td>0.628216</td>\n",
       "      <td>-0.519047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.011255</td>\n",
       "      <td>0.979333</td>\n",
       "      <td>110.628526</td>\n",
       "      <td>34.027214</td>\n",
       "      <td>158.526000</td>\n",
       "      <td>0.400545</td>\n",
       "      <td>12272.698085</td>\n",
       "      <td>0.664876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.909259</td>\n",
       "      <td>-0.266255</td>\n",
       "      <td>-0.190852</td>\n",
       "      <td>-0.650058</td>\n",
       "      <td>0.227760</td>\n",
       "      <td>-0.084634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0.713889</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>1.133750</td>\n",
       "      <td>81.314539</td>\n",
       "      <td>855.953316</td>\n",
       "      <td>125.234375</td>\n",
       "      <td>0.551199</td>\n",
       "      <td>7468.007642</td>\n",
       "      <td>0.791514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.056693</td>\n",
       "      <td>0.030405</td>\n",
       "      <td>0.521097</td>\n",
       "      <td>0.436343</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.377494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.423279</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.647143</td>\n",
       "      <td>76.987798</td>\n",
       "      <td>1185.787163</td>\n",
       "      <td>111.768714</td>\n",
       "      <td>0.196083</td>\n",
       "      <td>7112.908141</td>\n",
       "      <td>0.414497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.485218</td>\n",
       "      <td>-0.589304</td>\n",
       "      <td>-0.324427</td>\n",
       "      <td>-0.771878</td>\n",
       "      <td>0.554186</td>\n",
       "      <td>-0.653666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>0.173923</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>8.695493</td>\n",
       "      <td>15.643016</td>\n",
       "      <td>12.686000</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>91.254622</td>\n",
       "      <td>0.190489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.354372</td>\n",
       "      <td>-0.580518</td>\n",
       "      <td>-0.967126</td>\n",
       "      <td>0.743021</td>\n",
       "      <td>0.851538</td>\n",
       "      <td>0.281819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.996881</td>\n",
       "      <td>0.994254</td>\n",
       "      <td>-0.999601</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>-0.094058</td>\n",
       "      <td>0.993838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079607</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>20.820096</td>\n",
       "      <td>70.686825</td>\n",
       "      <td>31.227250</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>504.163218</td>\n",
       "      <td>0.089860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.974909</td>\n",
       "      <td>-0.693112</td>\n",
       "      <td>0.743290</td>\n",
       "      <td>0.970112</td>\n",
       "      <td>-0.978075</td>\n",
       "      <td>-0.998868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033449</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>11.700251</td>\n",
       "      <td>12.732268</td>\n",
       "      <td>17.873500</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>149.628145</td>\n",
       "      <td>0.036061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.551526</td>\n",
       "      <td>0.902205</td>\n",
       "      <td>-0.630369</td>\n",
       "      <td>0.656738</td>\n",
       "      <td>-0.447973</td>\n",
       "      <td>-0.294161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>0.346295</td>\n",
       "      <td>0.061131</td>\n",
       "      <td>0.515429</td>\n",
       "      <td>78.525909</td>\n",
       "      <td>1012.970723</td>\n",
       "      <td>115.185000</td>\n",
       "      <td>0.181051</td>\n",
       "      <td>7179.289049</td>\n",
       "      <td>0.232164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.657767</td>\n",
       "      <td>-0.450311</td>\n",
       "      <td>0.221672</td>\n",
       "      <td>0.259118</td>\n",
       "      <td>-0.827505</td>\n",
       "      <td>-0.607677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1</td>\n",
       "      <td>0.198766</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>22.740215</td>\n",
       "      <td>104.237177</td>\n",
       "      <td>37.481750</td>\n",
       "      <td>0.045188</td>\n",
       "      <td>621.354575</td>\n",
       "      <td>0.161190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.033289</td>\n",
       "      <td>-0.068947</td>\n",
       "      <td>0.519066</td>\n",
       "      <td>-0.643627</td>\n",
       "      <td>0.161396</td>\n",
       "      <td>-0.553858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0</td>\n",
       "      <td>0.159705</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>21.339074</td>\n",
       "      <td>149.172693</td>\n",
       "      <td>34.638800</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>604.528755</td>\n",
       "      <td>0.158401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.376666</td>\n",
       "      <td>-0.338193</td>\n",
       "      <td>-0.733423</td>\n",
       "      <td>0.713995</td>\n",
       "      <td>-0.818308</td>\n",
       "      <td>-0.480116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0</td>\n",
       "      <td>0.126710</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>9.389289</td>\n",
       "      <td>19.379696</td>\n",
       "      <td>14.033000</td>\n",
       "      <td>0.017645</td>\n",
       "      <td>107.538453</td>\n",
       "      <td>0.124137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.922585</td>\n",
       "      <td>0.752121</td>\n",
       "      <td>-0.490738</td>\n",
       "      <td>-0.538151</td>\n",
       "      <td>-0.397562</td>\n",
       "      <td>-0.254389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0</td>\n",
       "      <td>0.330524</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>0.489333</td>\n",
       "      <td>61.084126</td>\n",
       "      <td>489.711138</td>\n",
       "      <td>92.093833</td>\n",
       "      <td>0.127878</td>\n",
       "      <td>4220.981569</td>\n",
       "      <td>0.297024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.075175</td>\n",
       "      <td>0.558130</td>\n",
       "      <td>0.436084</td>\n",
       "      <td>-0.275042</td>\n",
       "      <td>-0.681541</td>\n",
       "      <td>0.672870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1</td>\n",
       "      <td>0.153154</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.222714</td>\n",
       "      <td>21.833134</td>\n",
       "      <td>185.507838</td>\n",
       "      <td>34.778143</td>\n",
       "      <td>0.025492</td>\n",
       "      <td>662.193580</td>\n",
       "      <td>0.146034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.082874</td>\n",
       "      <td>0.286534</td>\n",
       "      <td>0.801087</td>\n",
       "      <td>-0.155282</td>\n",
       "      <td>-0.590617</td>\n",
       "      <td>0.551972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0</td>\n",
       "      <td>0.125270</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.162250</td>\n",
       "      <td>6.067792</td>\n",
       "      <td>3.538359</td>\n",
       "      <td>9.594000</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>40.356465</td>\n",
       "      <td>0.125634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.829899</td>\n",
       "      <td>-0.477114</td>\n",
       "      <td>-0.708478</td>\n",
       "      <td>0.907183</td>\n",
       "      <td>0.041469</td>\n",
       "      <td>-0.226795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0</td>\n",
       "      <td>0.090058</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.118250</td>\n",
       "      <td>6.330384</td>\n",
       "      <td>1.601982</td>\n",
       "      <td>9.689000</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>41.675741</td>\n",
       "      <td>0.089765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.612982</td>\n",
       "      <td>0.877795</td>\n",
       "      <td>-0.571765</td>\n",
       "      <td>0.580842</td>\n",
       "      <td>-0.508578</td>\n",
       "      <td>-0.952612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0</td>\n",
       "      <td>0.383723</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>65.656112</td>\n",
       "      <td>824.520263</td>\n",
       "      <td>98.881400</td>\n",
       "      <td>0.221725</td>\n",
       "      <td>5135.245289</td>\n",
       "      <td>0.260703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.373476</td>\n",
       "      <td>-0.110967</td>\n",
       "      <td>-0.857493</td>\n",
       "      <td>-0.504313</td>\n",
       "      <td>0.748260</td>\n",
       "      <td>-0.427765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1</td>\n",
       "      <td>0.164924</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.242833</td>\n",
       "      <td>21.362044</td>\n",
       "      <td>66.411931</td>\n",
       "      <td>31.928833</td>\n",
       "      <td>0.030792</td>\n",
       "      <td>522.748862</td>\n",
       "      <td>0.146652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.113066</td>\n",
       "      <td>0.296897</td>\n",
       "      <td>0.690521</td>\n",
       "      <td>-0.711697</td>\n",
       "      <td>0.187383</td>\n",
       "      <td>-0.219321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0</td>\n",
       "      <td>0.247395</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>57.128342</td>\n",
       "      <td>1593.442819</td>\n",
       "      <td>83.257600</td>\n",
       "      <td>0.076198</td>\n",
       "      <td>4857.090231</td>\n",
       "      <td>0.210699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.705281</td>\n",
       "      <td>-0.665206</td>\n",
       "      <td>-0.010481</td>\n",
       "      <td>0.492941</td>\n",
       "      <td>-0.950752</td>\n",
       "      <td>-0.668314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166666</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>48.365651</td>\n",
       "      <td>622.950587</td>\n",
       "      <td>75.523600</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>2962.186812</td>\n",
       "      <td>0.138845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.211986</td>\n",
       "      <td>0.303777</td>\n",
       "      <td>0.815085</td>\n",
       "      <td>-0.583473</td>\n",
       "      <td>-0.033446</td>\n",
       "      <td>-0.308316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0</td>\n",
       "      <td>0.284173</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>0.435750</td>\n",
       "      <td>98.983189</td>\n",
       "      <td>2046.098536</td>\n",
       "      <td>133.037750</td>\n",
       "      <td>0.089705</td>\n",
       "      <td>11843.770182</td>\n",
       "      <td>0.257560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.275627</td>\n",
       "      <td>0.039709</td>\n",
       "      <td>-0.513060</td>\n",
       "      <td>-0.481017</td>\n",
       "      <td>-0.064881</td>\n",
       "      <td>0.893067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0.190856</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>40.730712</td>\n",
       "      <td>507.323091</td>\n",
       "      <td>56.958000</td>\n",
       "      <td>0.040858</td>\n",
       "      <td>2166.313981</td>\n",
       "      <td>0.176185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.462633</td>\n",
       "      <td>0.558953</td>\n",
       "      <td>0.972490</td>\n",
       "      <td>-0.756603</td>\n",
       "      <td>-0.319999</td>\n",
       "      <td>-0.196957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0</td>\n",
       "      <td>0.343901</td>\n",
       "      <td>0.037910</td>\n",
       "      <td>0.500750</td>\n",
       "      <td>95.781538</td>\n",
       "      <td>3163.793540</td>\n",
       "      <td>126.374750</td>\n",
       "      <td>0.156178</td>\n",
       "      <td>12337.896489</td>\n",
       "      <td>0.251585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.699509</td>\n",
       "      <td>-0.173218</td>\n",
       "      <td>-0.323138</td>\n",
       "      <td>-0.952513</td>\n",
       "      <td>0.968374</td>\n",
       "      <td>-0.986838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>0.194691</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>40.855848</td>\n",
       "      <td>277.531765</td>\n",
       "      <td>55.927000</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>1946.732074</td>\n",
       "      <td>0.194691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.068641</td>\n",
       "      <td>-0.079981</td>\n",
       "      <td>0.867227</td>\n",
       "      <td>-0.434980</td>\n",
       "      <td>-0.509555</td>\n",
       "      <td>-0.234310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0</td>\n",
       "      <td>0.630203</td>\n",
       "      <td>0.080833</td>\n",
       "      <td>0.938833</td>\n",
       "      <td>142.300996</td>\n",
       "      <td>5049.081602</td>\n",
       "      <td>210.001167</td>\n",
       "      <td>0.477988</td>\n",
       "      <td>25298.654940</td>\n",
       "      <td>0.578942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.815394</td>\n",
       "      <td>-0.780397</td>\n",
       "      <td>-0.995427</td>\n",
       "      <td>-0.936491</td>\n",
       "      <td>0.879099</td>\n",
       "      <td>-0.695077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>2</td>\n",
       "      <td>0.626514</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>0.984857</td>\n",
       "      <td>124.500367</td>\n",
       "      <td>2076.233452</td>\n",
       "      <td>195.101000</td>\n",
       "      <td>0.451294</td>\n",
       "      <td>17576.574843</td>\n",
       "      <td>0.646205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.080352</td>\n",
       "      <td>0.423364</td>\n",
       "      <td>0.425622</td>\n",
       "      <td>-0.973346</td>\n",
       "      <td>0.533496</td>\n",
       "      <td>-0.556987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>2</td>\n",
       "      <td>0.820999</td>\n",
       "      <td>0.082737</td>\n",
       "      <td>1.243000</td>\n",
       "      <td>166.087013</td>\n",
       "      <td>1060.360669</td>\n",
       "      <td>255.258000</td>\n",
       "      <td>0.756777</td>\n",
       "      <td>28645.256684</td>\n",
       "      <td>0.739893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.403011</td>\n",
       "      <td>0.498782</td>\n",
       "      <td>0.333231</td>\n",
       "      <td>0.537758</td>\n",
       "      <td>-0.460595</td>\n",
       "      <td>-0.451621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>2</td>\n",
       "      <td>0.900506</td>\n",
       "      <td>0.036022</td>\n",
       "      <td>1.371200</td>\n",
       "      <td>172.289229</td>\n",
       "      <td>4206.418423</td>\n",
       "      <td>272.794800</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>33889.996816</td>\n",
       "      <td>0.985751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.287175</td>\n",
       "      <td>-0.711580</td>\n",
       "      <td>-0.293698</td>\n",
       "      <td>0.126773</td>\n",
       "      <td>-0.351123</td>\n",
       "      <td>-0.907420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>2</td>\n",
       "      <td>1.047733</td>\n",
       "      <td>0.053141</td>\n",
       "      <td>1.634800</td>\n",
       "      <td>170.619286</td>\n",
       "      <td>689.359381</td>\n",
       "      <td>227.098600</td>\n",
       "      <td>1.150886</td>\n",
       "      <td>29800.300241</td>\n",
       "      <td>1.106429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800312</td>\n",
       "      <td>-0.297677</td>\n",
       "      <td>0.058636</td>\n",
       "      <td>-0.189713</td>\n",
       "      <td>0.336595</td>\n",
       "      <td>-0.768215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>2</td>\n",
       "      <td>1.079017</td>\n",
       "      <td>0.081148</td>\n",
       "      <td>1.658750</td>\n",
       "      <td>234.421847</td>\n",
       "      <td>2672.984210</td>\n",
       "      <td>357.965250</td>\n",
       "      <td>1.245427</td>\n",
       "      <td>57626.586372</td>\n",
       "      <td>1.103559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.491571</td>\n",
       "      <td>-0.808745</td>\n",
       "      <td>-0.154482</td>\n",
       "      <td>-0.458424</td>\n",
       "      <td>0.344757</td>\n",
       "      <td>-0.961717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2</td>\n",
       "      <td>1.173821</td>\n",
       "      <td>0.197109</td>\n",
       "      <td>1.696800</td>\n",
       "      <td>205.441512</td>\n",
       "      <td>3458.064623</td>\n",
       "      <td>300.416600</td>\n",
       "      <td>1.574964</td>\n",
       "      <td>45664.279299</td>\n",
       "      <td>1.319540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.635831</td>\n",
       "      <td>-0.727625</td>\n",
       "      <td>-0.088087</td>\n",
       "      <td>-0.501645</td>\n",
       "      <td>0.541838</td>\n",
       "      <td>-0.843833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>2</td>\n",
       "      <td>0.366393</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>116.636072</td>\n",
       "      <td>2142.566423</td>\n",
       "      <td>166.238600</td>\n",
       "      <td>0.139494</td>\n",
       "      <td>15746.539803</td>\n",
       "      <td>0.365197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.681096</td>\n",
       "      <td>-0.402148</td>\n",
       "      <td>0.047498</td>\n",
       "      <td>-0.787296</td>\n",
       "      <td>0.663284</td>\n",
       "      <td>-0.783834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>2</td>\n",
       "      <td>0.121882</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>29.971873</td>\n",
       "      <td>75.563107</td>\n",
       "      <td>47.688200</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>973.876257</td>\n",
       "      <td>0.144997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.693160</td>\n",
       "      <td>-0.106090</td>\n",
       "      <td>-0.596451</td>\n",
       "      <td>-0.055153</td>\n",
       "      <td>-0.533600</td>\n",
       "      <td>-0.796788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2</td>\n",
       "      <td>0.129176</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.191429</td>\n",
       "      <td>23.607732</td>\n",
       "      <td>0.872637</td>\n",
       "      <td>33.691286</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>558.197667</td>\n",
       "      <td>0.134402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.469730</td>\n",
       "      <td>0.250513</td>\n",
       "      <td>-0.321466</td>\n",
       "      <td>-0.638015</td>\n",
       "      <td>-0.780626</td>\n",
       "      <td>0.331929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hit_type     L_AAI     L_AVI    L_ASMA       L_GAI        L_GVI  \\\n",
       "0           1  0.144363  0.000984  0.182714    7.837218    16.855832   \n",
       "1           0  0.172753  0.005393  0.228000   31.572826   161.915005   \n",
       "2           0  0.152061  0.000366  0.226429   62.496385    28.197039   \n",
       "3           0  0.431467  0.090386  0.643800   56.532036   555.794405   \n",
       "4           1  0.390009  0.014918  0.629600   40.664224   350.779650   \n",
       "5           0  0.201691  0.003187  0.323400   42.791711    18.822723   \n",
       "6           0  0.127174  0.000095  0.206000   14.423266    37.821719   \n",
       "7           0  0.095510  0.000210  0.133600   11.464270     6.423106   \n",
       "8           1  0.152996  0.000584  0.222400   18.565111     3.307427   \n",
       "9           0  0.328453  0.018054  0.498600   53.995201  1544.527742   \n",
       "10          1  0.309602  0.028823  0.474667   41.886609   358.011245   \n",
       "11          0  0.194255  0.000122  0.305250   18.869593    15.297130   \n",
       "12          1  0.134342  0.000273  0.187333    9.563026     7.483308   \n",
       "13          0  0.098452  0.000051  0.147400   12.379005    26.802259   \n",
       "14          0  0.084664  0.000139  0.105000    5.559359     5.815340   \n",
       "15          0  0.077464  0.000227  0.093250    8.528079     6.506929   \n",
       "16          1  0.097237  0.000517  0.139571   13.555943    25.983775   \n",
       "17          0  0.102223  0.000179  0.142429   11.472766    13.361363   \n",
       "18          0  0.088157  0.002541  0.116714   16.533870    15.443233   \n",
       "19          0  0.360621  0.019851  0.559000   84.131220  1144.326018   \n",
       "20          1  0.293364  0.001325  0.468000   62.188938    85.000956   \n",
       "21          0  0.270309  0.002249  0.425750   33.561496   383.309644   \n",
       "22          0  0.163944  0.000326  0.227167    9.197816     7.981695   \n",
       "23          0  0.260209  0.003585  0.398500   19.859446   468.626553   \n",
       "24          2  0.564537  0.009410  0.815400  100.073674   786.448232   \n",
       "25          2  0.752399  0.036944  1.136833   86.229255   436.178918   \n",
       "26          2  0.657002  0.010955  0.980000  157.180415  4321.316139   \n",
       "27          2  0.623931  0.011255  0.979333  110.628526    34.027214   \n",
       "28          2  0.713889  0.041561  1.133750   81.314539   855.953316   \n",
       "29          2  0.423279  0.016918  0.647143   76.987798  1185.787163   \n",
       "..        ...       ...       ...       ...         ...          ...   \n",
       "331         0  0.173923  0.000891  0.256600    8.695493    15.643016   \n",
       "332         1       NaN       NaN       NaN         NaN          NaN   \n",
       "333         0  0.079607  0.000638  0.110500   20.820096    70.686825   \n",
       "334         0  0.033449  0.000064  0.046000   11.700251    12.732268   \n",
       "335         0  0.346295  0.061131  0.515429   78.525909  1012.970723   \n",
       "336         1  0.198766  0.005681  0.290500   22.740215   104.237177   \n",
       "337         0  0.159705  0.000565  0.216800   21.339074   149.172693   \n",
       "338         0  0.126710  0.001590  0.161800    9.389289    19.379696   \n",
       "339         0  0.330524  0.018631  0.489333   61.084126   489.711138   \n",
       "340         1  0.153154  0.002036  0.222714   21.833134   185.507838   \n",
       "341         0  0.125270  0.000243  0.162250    6.067792     3.538359   \n",
       "342         0  0.090058  0.000227  0.118250    6.330384     1.601982   \n",
       "343         0  0.383723  0.074482  0.573200   65.656112   824.520263   \n",
       "344         1  0.164924  0.003592  0.242833   21.362044    66.411931   \n",
       "345         0  0.247395  0.014994  0.387200   57.128342  1593.442819   \n",
       "346         1  0.166666  0.007380  0.261200   48.365651   622.950587   \n",
       "347         0  0.284173  0.008951  0.435750   98.983189  2046.098536   \n",
       "348         1  0.190856  0.004432  0.302000   40.730712   507.323091   \n",
       "349         0  0.343901  0.037910  0.500750   95.781538  3163.793540   \n",
       "350         1  0.194691  0.001110  0.205500   40.855848   277.531765   \n",
       "351         0  0.630203  0.080833  0.938833  142.300996  5049.081602   \n",
       "352         2  0.626514  0.058773  0.984857  124.500367  2076.233452   \n",
       "353         2  0.820999  0.082737  1.243000  166.087013  1060.360669   \n",
       "354         2  0.900506  0.036022  1.371200  172.289229  4206.418423   \n",
       "355         2  1.047733  0.053141  1.634800  170.619286   689.359381   \n",
       "356         2  1.079017  0.081148  1.658750  234.421847  2672.984210   \n",
       "357         2  1.173821  0.197109  1.696800  205.441512  3458.064623   \n",
       "358         2  0.366393  0.005251  0.582400  116.636072  2142.566423   \n",
       "359         2  0.121882  0.001987  0.178400   29.971873    75.563107   \n",
       "360         2  0.129176  0.000246  0.191429   23.607732     0.872637   \n",
       "\n",
       "         L_GSMA     L_AAE         L_ARE    L_MAMI    ...        R_AZCR  \\\n",
       "0     11.142000  0.021825     78.277812  0.126178    ...      0.428571   \n",
       "1     47.817571  0.035236   1158.758338  0.183308    ...      0.142857   \n",
       "2     79.462143  0.023488   3933.995127  0.156170    ...      0.750000   \n",
       "3     83.879000  0.276550   3751.665457  0.233283    ...      0.333333   \n",
       "4     63.690200  0.167024   2004.358734  0.346880    ...      0.375000   \n",
       "5     72.489400  0.043866   1849.953245  0.185637    ...      0.285714   \n",
       "6     23.181800  0.016269    245.852329  0.124161    ...      0.375000   \n",
       "7     17.134200  0.009332    137.852601  0.087926    ...      0.500000   \n",
       "8     27.880600  0.023992    347.970792  0.149114    ...      0.285714   \n",
       "9     80.151000  0.125935   4460.009459  0.299490    ...      0.428571   \n",
       "10    68.174000  0.124677   2112.499227  0.231707    ...      0.428571   \n",
       "11    29.930250  0.037857    371.358653  0.197707    ...      0.400000   \n",
       "12    14.086833  0.018320     98.934765  0.135733    ...      0.285714   \n",
       "13    19.109800  0.009743    180.042036  0.099479    ...      0.142857   \n",
       "14     8.936250  0.007306     36.721813  0.082901    ...      0.428571   \n",
       "15    12.662500  0.006228     79.235062  0.075244    ...      0.500000   \n",
       "16    20.488857  0.009972    209.747375  0.104484    ...      0.142857   \n",
       "17    17.047286  0.010628    144.985724  0.101400    ...      0.250000   \n",
       "18    24.418571  0.010313    288.812074  0.122057    ...      0.125000   \n",
       "19   125.163250  0.149899   8222.388277  0.378034    ...      0.400000   \n",
       "20    98.146600  0.087387   3952.464974  0.311443    ...      0.166667   \n",
       "21    55.995750  0.075316   1509.683663  0.271377    ...      0.428571   \n",
       "22    13.277167  0.027204     92.581520  0.162121    ...      0.285714   \n",
       "23    30.719750  0.071294    863.024136  0.251451    ...      0.285714   \n",
       "24   151.047600  0.328111  10801.188459  0.583504    ...      0.285714   \n",
       "25   129.587000  0.603048   7871.663289  0.808732    ...      0.250000   \n",
       "26   244.170750  0.442607  29026.998923  0.693152    ...      0.375000   \n",
       "27   158.526000  0.400545  12272.698085  0.664876    ...      0.600000   \n",
       "28   125.234375  0.551199   7468.007642  0.791514    ...      0.500000   \n",
       "29   111.768714  0.196083   7112.908141  0.414497    ...      0.285714   \n",
       "..          ...       ...           ...       ...    ...           ...   \n",
       "331   12.686000  0.031140     91.254622  0.190489    ...      0.666667   \n",
       "332         NaN       NaN           NaN       NaN    ...      0.666667   \n",
       "333   31.227250  0.006975    504.163218  0.089860    ...      0.500000   \n",
       "334   17.873500  0.001183    149.628145  0.036061    ...      0.142857   \n",
       "335  115.185000  0.181051   7179.289049  0.232164    ...      0.400000   \n",
       "336   37.481750  0.045188    621.354575  0.161190    ...      0.166667   \n",
       "337   34.638800  0.026071    604.528755  0.158401    ...      0.428571   \n",
       "338   14.033000  0.017645    107.538453  0.124137    ...      0.428571   \n",
       "339   92.093833  0.127878   4220.981569  0.297024    ...      0.142857   \n",
       "340   34.778143  0.025492    662.193580  0.146034    ...      0.166667   \n",
       "341    9.594000  0.015936     40.356465  0.125634    ...      0.250000   \n",
       "342    9.689000  0.008337     41.675741  0.089765    ...      0.666667   \n",
       "343   98.881400  0.221725   5135.245289  0.260703    ...      0.500000   \n",
       "344   31.928833  0.030792    522.748862  0.146652    ...      0.375000   \n",
       "345   83.257600  0.076198   4857.090231  0.210699    ...      0.142857   \n",
       "346   75.523600  0.035157   2962.186812  0.138845    ...      0.375000   \n",
       "347  133.037750  0.089705  11843.770182  0.257560    ...      0.500000   \n",
       "348   56.958000  0.040858   2166.313981  0.176185    ...      0.428571   \n",
       "349  126.374750  0.156178  12337.896489  0.251585    ...      0.400000   \n",
       "350   55.927000  0.039014   1946.732074  0.194691    ...      0.571429   \n",
       "351  210.001167  0.477988  25298.654940  0.578942    ...      0.600000   \n",
       "352  195.101000  0.451294  17576.574843  0.646205    ...      0.250000   \n",
       "353  255.258000  0.756777  28645.256684  0.739893    ...      0.125000   \n",
       "354  272.794800  0.846933  33889.996816  0.985751    ...      0.428571   \n",
       "355  227.098600  1.150886  29800.300241  1.106429    ...      0.333333   \n",
       "356  357.965250  1.245427  57626.586372  1.103559    ...      0.714286   \n",
       "357  300.416600  1.574964  45664.279299  1.319540    ...      0.571429   \n",
       "358  166.238600  0.139494  15746.539803  0.365197    ...      0.375000   \n",
       "359   47.688200  0.016842    973.876257  0.144997    ...      0.142857   \n",
       "360   33.691286  0.016932    558.197667  0.134402    ...      0.428571   \n",
       "\n",
       "       R_GZCR    R_AMCR    R_GMCR  R_AXYCORR  R_AYZCORR  R_AZXCORR  R_GXYCORR  \\\n",
       "0    0.571429  0.285714  0.571429  -0.544855  -0.684912   0.121344   0.752108   \n",
       "1    0.285714  0.142857  0.285714  -0.185425  -0.418489   0.697969  -0.488330   \n",
       "2    0.250000  0.750000  0.250000  -0.786548   0.911355  -0.795170   0.975555   \n",
       "3    0.333333  0.333333  0.333333   0.835047  -0.485918   0.075094  -0.244019   \n",
       "4    0.375000  0.375000  0.375000   0.016678  -0.060541  -0.043862  -0.886426   \n",
       "5    0.285714  0.142857  0.285714   0.487971   0.100405  -0.038495   0.689536   \n",
       "6    0.125000  0.375000  0.125000  -0.660409   0.870875  -0.813031  -0.115209   \n",
       "7    0.333333  0.166667  0.333333  -0.865026  -0.689175   0.676779  -0.055290   \n",
       "8    0.571429  0.285714  0.571429  -0.669668  -0.409309   0.522083  -0.256624   \n",
       "9    0.142857  0.428571  0.142857   0.170319   0.921414   0.224085  -0.725166   \n",
       "10   0.428571  0.428571  0.285714   0.409492  -0.904054  -0.127712   0.795752   \n",
       "11   0.200000  0.400000  0.200000  -0.073651  -0.706916  -0.474636   0.384338   \n",
       "12   0.428571  0.285714  0.571429  -0.235666   0.333801   0.437924  -0.058161   \n",
       "13   0.428571  0.142857  0.428571   0.214161   0.489343  -0.631098  -0.402869   \n",
       "14   0.285714  0.285714  0.285714  -0.442770  -0.594044   0.275310   0.977695   \n",
       "15   0.500000  0.500000  0.500000  -0.908620  -0.331470  -0.092264   0.117552   \n",
       "16   0.285714  0.285714  0.285714  -0.231539  -0.017266   0.135815   0.285040   \n",
       "17   0.500000  0.250000  0.250000  -0.449289   0.764545  -0.831028  -0.974738   \n",
       "18   0.375000  0.125000  0.375000   0.476185   0.669930   0.311848   0.852547   \n",
       "19   0.400000  0.200000  0.200000   0.524693   0.597842  -0.147682   0.268676   \n",
       "20   0.666667  0.166667  0.666667   0.300233  -0.307793  -0.799677  -0.139533   \n",
       "21   0.142857  0.571429  0.142857   0.546529  -0.325672  -0.210766   0.522460   \n",
       "22   0.285714  0.285714  0.428571  -0.329155   0.202903  -0.893932   0.191525   \n",
       "23   0.142857  0.142857  0.142857  -0.537622   0.132907  -0.672478  -0.289827   \n",
       "24   0.714286  0.285714  0.714286  -0.631941  -0.125805   0.081967   0.319282   \n",
       "25   0.750000  0.250000  0.750000  -0.425767  -0.708604   0.605424  -0.925964   \n",
       "26   0.375000  0.375000  0.375000   0.556245  -0.258675   0.198119  -0.672692   \n",
       "27   0.400000  0.400000  0.400000   0.909259  -0.266255  -0.190852  -0.650058   \n",
       "28   0.500000  0.500000  0.333333   0.056693   0.030405   0.521097   0.436343   \n",
       "29   0.285714  0.285714  0.285714   0.485218  -0.589304  -0.324427  -0.771878   \n",
       "..        ...       ...       ...        ...        ...        ...        ...   \n",
       "331  0.333333  0.666667  0.333333   0.354372  -0.580518  -0.967126   0.743021   \n",
       "332  0.666667  0.666667  0.666667  -0.996881   0.994254  -0.999601   0.016872   \n",
       "333  0.250000  0.500000  0.250000  -0.974909  -0.693112   0.743290   0.970112   \n",
       "334  0.285714  0.142857  0.142857  -0.551526   0.902205  -0.630369   0.656738   \n",
       "335  0.400000  0.200000  0.200000  -0.657767  -0.450311   0.221672   0.259118   \n",
       "336  0.166667  0.166667  0.166667  -0.033289  -0.068947   0.519066  -0.643627   \n",
       "337  0.285714  0.285714  0.142857   0.376666  -0.338193  -0.733423   0.713995   \n",
       "338  0.428571  0.142857  0.142857  -0.922585   0.752121  -0.490738  -0.538151   \n",
       "339  0.285714  0.142857  0.142857  -0.075175   0.558130   0.436084  -0.275042   \n",
       "340  0.166667  0.166667  0.166667  -0.082874   0.286534   0.801087  -0.155282   \n",
       "341  0.125000  0.250000  0.125000   0.829899  -0.477114  -0.708478   0.907183   \n",
       "342  0.500000  0.666667  0.166667  -0.612982   0.877795  -0.571765   0.580842   \n",
       "343  0.500000  0.500000  0.500000   0.373476  -0.110967  -0.857493  -0.504313   \n",
       "344  0.125000  0.375000  0.125000  -0.113066   0.296897   0.690521  -0.711697   \n",
       "345  0.428571  0.142857  0.142857  -0.705281  -0.665206  -0.010481   0.492941   \n",
       "346  0.125000  0.375000  0.125000  -0.211986   0.303777   0.815085  -0.583473   \n",
       "347  0.500000  0.500000  0.500000   0.275627   0.039709  -0.513060  -0.481017   \n",
       "348  0.142857  0.428571  0.142857   0.462633   0.558953   0.972490  -0.756603   \n",
       "349  0.400000  0.200000  0.400000  -0.699509  -0.173218  -0.323138  -0.952513   \n",
       "350  0.285714  0.285714  0.285714   0.068641  -0.079981   0.867227  -0.434980   \n",
       "351  0.400000  0.200000  0.400000   0.815394  -0.780397  -0.995427  -0.936491   \n",
       "352  0.125000  0.250000  0.125000   0.080352   0.423364   0.425622  -0.973346   \n",
       "353  0.500000  0.125000  0.750000  -0.403011   0.498782   0.333231   0.537758   \n",
       "354  0.428571  0.571429  0.428571  -0.287175  -0.711580  -0.293698   0.126773   \n",
       "355  0.500000  0.333333  0.500000   0.800312  -0.297677   0.058636  -0.189713   \n",
       "356  0.714286  0.285714  0.714286   0.491571  -0.808745  -0.154482  -0.458424   \n",
       "357  0.571429  0.571429  0.571429   0.635831  -0.727625  -0.088087  -0.501645   \n",
       "358  0.500000  0.375000  0.625000   0.681096  -0.402148   0.047498  -0.787296   \n",
       "359  0.285714  0.142857  0.285714   0.693160  -0.106090  -0.596451  -0.055153   \n",
       "360  0.142857  0.428571  0.142857   0.469730   0.250513  -0.321466  -0.638015   \n",
       "\n",
       "     R_GYZCORR  R_GZXCORR  \n",
       "0     0.457800  -0.055694  \n",
       "1     0.925166  -0.747645  \n",
       "2    -0.810204  -0.866550  \n",
       "3     0.141725  -0.994565  \n",
       "4     0.493714  -0.532830  \n",
       "5    -0.754955  -0.864607  \n",
       "6    -0.012770   0.833729  \n",
       "7     0.310975   0.007419  \n",
       "8    -0.473077   0.119202  \n",
       "9     0.809888  -0.624116  \n",
       "10   -0.663151  -0.788868  \n",
       "11   -0.732997  -0.313854  \n",
       "12   -0.157697   0.574602  \n",
       "13    0.334590   0.099094  \n",
       "14   -0.953086  -0.992685  \n",
       "15   -0.214236  -0.949340  \n",
       "16   -0.563274   0.278315  \n",
       "17    0.925727  -0.903211  \n",
       "18   -0.590252  -0.803097  \n",
       "19    0.534829  -0.639858  \n",
       "20    0.461526  -0.638487  \n",
       "21    0.729067   0.732887  \n",
       "22   -0.917464  -0.489747  \n",
       "23   -0.417150   0.372505  \n",
       "24    0.519681   0.243485  \n",
       "25    0.772844  -0.813119  \n",
       "26    0.628216  -0.519047  \n",
       "27    0.227760  -0.084634  \n",
       "28    0.265100   0.377494  \n",
       "29    0.554186  -0.653666  \n",
       "..         ...        ...  \n",
       "331   0.851538   0.281819  \n",
       "332  -0.094058   0.993838  \n",
       "333  -0.978075  -0.998868  \n",
       "334  -0.447973  -0.294161  \n",
       "335  -0.827505  -0.607677  \n",
       "336   0.161396  -0.553858  \n",
       "337  -0.818308  -0.480116  \n",
       "338  -0.397562  -0.254389  \n",
       "339  -0.681541   0.672870  \n",
       "340  -0.590617   0.551972  \n",
       "341   0.041469  -0.226795  \n",
       "342  -0.508578  -0.952612  \n",
       "343   0.748260  -0.427765  \n",
       "344   0.187383  -0.219321  \n",
       "345  -0.950752  -0.668314  \n",
       "346  -0.033446  -0.308316  \n",
       "347  -0.064881   0.893067  \n",
       "348  -0.319999  -0.196957  \n",
       "349   0.968374  -0.986838  \n",
       "350  -0.509555  -0.234310  \n",
       "351   0.879099  -0.695077  \n",
       "352   0.533496  -0.556987  \n",
       "353  -0.460595  -0.451621  \n",
       "354  -0.351123  -0.907420  \n",
       "355   0.336595  -0.768215  \n",
       "356   0.344757  -0.961717  \n",
       "357   0.541838  -0.843833  \n",
       "358   0.663284  -0.783834  \n",
       "359  -0.533600  -0.796788  \n",
       "360  -0.780626   0.331929  \n",
       "\n",
       "[361 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfs[0][2].event_primitive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:28:26.336986Z",
     "start_time": "2018-07-26T07:28:26.333460Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare(train_who, train_porder, valid_porder):\n",
    "    train_df = pfs[train_who - 1][train_porder - 1].event_primitive_df\n",
    "    x = train_df.drop(['hit_type'], axis=1)\n",
    "    y = train_df['hit_type']\n",
    "    \n",
    "    valid_df = pfs[train_who - 1][valid_porder - 1].event_primitive_df\n",
    "    x_valid = valid_df.drop(['hit_type'], axis=1)\n",
    "    y_valid = valid_df['hit_type']\n",
    "    \n",
    "    return x, y, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:28:26.366283Z",
     "start_time": "2018-07-26T07:28:26.338336Z"
    }
   },
   "outputs": [],
   "source": [
    "params = dict({\n",
    "    'learning_rate': 0.5,\n",
    "    'application': 'multiclass',\n",
    "    'num_classes': 3,\n",
    "#     'min_data_in_leaf': 5,\n",
    "#     'max_depth': 8,\n",
    "    'num_leaves': 2 ** 3,\n",
    "    'verbosity': 0,\n",
    "    'metric': 'multi_error'\n",
    "})\n",
    "\n",
    "grid_params = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [8, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:28:26.413840Z",
     "start_time": "2018-07-26T07:28:26.369093Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:28:26.469697Z",
     "start_time": "2018-07-26T07:28:26.416728Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_multi_error(y, pred_y):\n",
    "    cnf_matrix = confusion_matrix(y, pred_y)\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "    cm = plot_confusion_matrix(cnf_matrix, classes=[0, 1, 2], normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    return sum([1 - cm[i][i] for i in range(cm.shape[0])]) / cm.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:55:19.012725Z",
     "start_time": "2018-07-26T07:55:03.057790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.127424\n",
      "[10]\tvalid_0's multi_error: 0.130194\n",
      "[15]\tvalid_0's multi_error: 0.124654\n",
      "[20]\tvalid_0's multi_error: 0.124654\n",
      "[25]\tvalid_0's multi_error: 0.121884\n",
      "[30]\tvalid_0's multi_error: 0.108033\n",
      "[35]\tvalid_0's multi_error: 0.108033\n",
      "[40]\tvalid_0's multi_error: 0.113573\n",
      "[45]\tvalid_0's multi_error: 0.113573\n",
      "[50]\tvalid_0's multi_error: 0.105263\n",
      "[55]\tvalid_0's multi_error: 0.105263\n",
      "[60]\tvalid_0's multi_error: 0.105263\n",
      "[65]\tvalid_0's multi_error: 0.105263\n",
      "[70]\tvalid_0's multi_error: 0.102493\n",
      "[75]\tvalid_0's multi_error: 0.102493\n",
      "[80]\tvalid_0's multi_error: 0.102493\n",
      "[85]\tvalid_0's multi_error: 0.102493\n",
      "[90]\tvalid_0's multi_error: 0.102493\n",
      "[95]\tvalid_0's multi_error: 0.102493\n",
      "[100]\tvalid_0's multi_error: 0.102493\n",
      "[105]\tvalid_0's multi_error: 0.102493\n",
      "[110]\tvalid_0's multi_error: 0.102493\n",
      "[115]\tvalid_0's multi_error: 0.102493\n",
      "[120]\tvalid_0's multi_error: 0.102493\n",
      "[125]\tvalid_0's multi_error: 0.102493\n",
      "[130]\tvalid_0's multi_error: 0.102493\n",
      "[135]\tvalid_0's multi_error: 0.102493\n",
      "[140]\tvalid_0's multi_error: 0.102493\n",
      "[145]\tvalid_0's multi_error: 0.102493\n",
      "[150]\tvalid_0's multi_error: 0.102493\n",
      "[155]\tvalid_0's multi_error: 0.105263\n",
      "[160]\tvalid_0's multi_error: 0.105263\n",
      "[165]\tvalid_0's multi_error: 0.105263\n",
      "[170]\tvalid_0's multi_error: 0.105263\n",
      "[175]\tvalid_0's multi_error: 0.105263\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's multi_error: 0.099723\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.141274\n",
      "[10]\tvalid_0's multi_error: 0.119114\n",
      "[15]\tvalid_0's multi_error: 0.119114\n",
      "[20]\tvalid_0's multi_error: 0.130194\n",
      "[25]\tvalid_0's multi_error: 0.130194\n",
      "[30]\tvalid_0's multi_error: 0.127424\n",
      "[35]\tvalid_0's multi_error: 0.124654\n",
      "[40]\tvalid_0's multi_error: 0.124654\n",
      "[45]\tvalid_0's multi_error: 0.124654\n",
      "[50]\tvalid_0's multi_error: 0.130194\n",
      "[55]\tvalid_0's multi_error: 0.138504\n",
      "[60]\tvalid_0's multi_error: 0.138504\n",
      "[65]\tvalid_0's multi_error: 0.132964\n",
      "[70]\tvalid_0's multi_error: 0.130194\n",
      "[75]\tvalid_0's multi_error: 0.132964\n",
      "[80]\tvalid_0's multi_error: 0.135734\n",
      "[85]\tvalid_0's multi_error: 0.141274\n",
      "[90]\tvalid_0's multi_error: 0.132964\n",
      "[95]\tvalid_0's multi_error: 0.138504\n",
      "[100]\tvalid_0's multi_error: 0.135734\n",
      "[105]\tvalid_0's multi_error: 0.144044\n",
      "[110]\tvalid_0's multi_error: 0.144044\n",
      "[115]\tvalid_0's multi_error: 0.141274\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's multi_error: 0.116343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.121884\n",
      "[10]\tvalid_0's multi_error: 0.108033\n",
      "[15]\tvalid_0's multi_error: 0.119114\n",
      "[20]\tvalid_0's multi_error: 0.116343\n",
      "[25]\tvalid_0's multi_error: 0.113573\n",
      "[30]\tvalid_0's multi_error: 0.119114\n",
      "[35]\tvalid_0's multi_error: 0.116343\n",
      "[40]\tvalid_0's multi_error: 0.116343\n",
      "[45]\tvalid_0's multi_error: 0.119114\n",
      "[50]\tvalid_0's multi_error: 0.113573\n",
      "[55]\tvalid_0's multi_error: 0.116343\n",
      "[60]\tvalid_0's multi_error: 0.119114\n",
      "[65]\tvalid_0's multi_error: 0.119114\n",
      "[70]\tvalid_0's multi_error: 0.116343\n",
      "[75]\tvalid_0's multi_error: 0.116343\n",
      "[80]\tvalid_0's multi_error: 0.113573\n",
      "[85]\tvalid_0's multi_error: 0.116343\n",
      "[90]\tvalid_0's multi_error: 0.119114\n",
      "[95]\tvalid_0's multi_error: 0.113573\n",
      "[100]\tvalid_0's multi_error: 0.108033\n",
      "[105]\tvalid_0's multi_error: 0.113573\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's multi_error: 0.105263\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.146814\n",
      "[10]\tvalid_0's multi_error: 0.124654\n",
      "[15]\tvalid_0's multi_error: 0.135734\n",
      "[20]\tvalid_0's multi_error: 0.135734\n",
      "[25]\tvalid_0's multi_error: 0.127424\n",
      "[30]\tvalid_0's multi_error: 0.119114\n",
      "[35]\tvalid_0's multi_error: 0.121884\n",
      "[40]\tvalid_0's multi_error: 0.124654\n",
      "[45]\tvalid_0's multi_error: 0.135734\n",
      "[50]\tvalid_0's multi_error: 0.138504\n",
      "[55]\tvalid_0's multi_error: 0.138504\n",
      "[60]\tvalid_0's multi_error: 0.132964\n",
      "[65]\tvalid_0's multi_error: 0.127424\n",
      "[70]\tvalid_0's multi_error: 0.124654\n",
      "[75]\tvalid_0's multi_error: 0.124654\n",
      "[80]\tvalid_0's multi_error: 0.127424\n",
      "[85]\tvalid_0's multi_error: 0.130194\n",
      "[90]\tvalid_0's multi_error: 0.127424\n",
      "[95]\tvalid_0's multi_error: 0.127424\n",
      "[100]\tvalid_0's multi_error: 0.127424\n",
      "[105]\tvalid_0's multi_error: 0.127424\n",
      "[110]\tvalid_0's multi_error: 0.127424\n",
      "[115]\tvalid_0's multi_error: 0.127424\n",
      "[120]\tvalid_0's multi_error: 0.127424\n",
      "[125]\tvalid_0's multi_error: 0.127424\n",
      "[130]\tvalid_0's multi_error: 0.127424\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's multi_error: 0.119114\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.182825\n",
      "[10]\tvalid_0's multi_error: 0.141274\n",
      "[15]\tvalid_0's multi_error: 0.149584\n",
      "[20]\tvalid_0's multi_error: 0.152355\n",
      "[25]\tvalid_0's multi_error: 0.144044\n",
      "[30]\tvalid_0's multi_error: 0.146814\n",
      "[35]\tvalid_0's multi_error: 0.155125\n",
      "[40]\tvalid_0's multi_error: 0.146814\n",
      "[45]\tvalid_0's multi_error: 0.146814\n",
      "[50]\tvalid_0's multi_error: 0.138504\n",
      "[55]\tvalid_0's multi_error: 0.138504\n",
      "[60]\tvalid_0's multi_error: 0.138504\n",
      "[65]\tvalid_0's multi_error: 0.138504\n",
      "[70]\tvalid_0's multi_error: 0.138504\n",
      "[75]\tvalid_0's multi_error: 0.138504\n",
      "[80]\tvalid_0's multi_error: 0.138504\n",
      "[85]\tvalid_0's multi_error: 0.141274\n",
      "[90]\tvalid_0's multi_error: 0.141274\n",
      "[95]\tvalid_0's multi_error: 0.146814\n",
      "[100]\tvalid_0's multi_error: 0.144044\n",
      "[105]\tvalid_0's multi_error: 0.146814\n",
      "[110]\tvalid_0's multi_error: 0.146814\n",
      "[115]\tvalid_0's multi_error: 0.146814\n",
      "[120]\tvalid_0's multi_error: 0.149584\n",
      "[125]\tvalid_0's multi_error: 0.146814\n",
      "[130]\tvalid_0's multi_error: 0.146814\n",
      "[135]\tvalid_0's multi_error: 0.146814\n",
      "[140]\tvalid_0's multi_error: 0.146814\n",
      "[145]\tvalid_0's multi_error: 0.146814\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's multi_error: 0.138504\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.144044\n",
      "[10]\tvalid_0's multi_error: 0.141274\n",
      "[15]\tvalid_0's multi_error: 0.138504\n",
      "[20]\tvalid_0's multi_error: 0.130194\n",
      "[25]\tvalid_0's multi_error: 0.119114\n",
      "[30]\tvalid_0's multi_error: 0.127424\n",
      "[35]\tvalid_0's multi_error: 0.130194\n",
      "[40]\tvalid_0's multi_error: 0.124654\n",
      "[45]\tvalid_0's multi_error: 0.116343\n",
      "[50]\tvalid_0's multi_error: 0.116343\n",
      "[55]\tvalid_0's multi_error: 0.121884\n",
      "[60]\tvalid_0's multi_error: 0.121884\n",
      "[65]\tvalid_0's multi_error: 0.119114\n",
      "[70]\tvalid_0's multi_error: 0.121884\n",
      "[75]\tvalid_0's multi_error: 0.119114\n",
      "[80]\tvalid_0's multi_error: 0.119114\n",
      "[85]\tvalid_0's multi_error: 0.121884\n",
      "[90]\tvalid_0's multi_error: 0.121884\n",
      "[95]\tvalid_0's multi_error: 0.121884\n",
      "[100]\tvalid_0's multi_error: 0.121884\n",
      "[105]\tvalid_0's multi_error: 0.116343\n",
      "[110]\tvalid_0's multi_error: 0.116343\n",
      "[115]\tvalid_0's multi_error: 0.116343\n",
      "[120]\tvalid_0's multi_error: 0.116343\n",
      "[125]\tvalid_0's multi_error: 0.116343\n",
      "[130]\tvalid_0's multi_error: 0.119114\n",
      "[135]\tvalid_0's multi_error: 0.119114\n",
      "[140]\tvalid_0's multi_error: 0.119114\n",
      "[145]\tvalid_0's multi_error: 0.119114\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's multi_error: 0.116343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.171745\n",
      "[10]\tvalid_0's multi_error: 0.163435\n",
      "[15]\tvalid_0's multi_error: 0.157895\n",
      "[20]\tvalid_0's multi_error: 0.160665\n",
      "[25]\tvalid_0's multi_error: 0.174515\n",
      "[30]\tvalid_0's multi_error: 0.174515\n",
      "[35]\tvalid_0's multi_error: 0.180055\n",
      "[40]\tvalid_0's multi_error: 0.185596\n",
      "[45]\tvalid_0's multi_error: 0.185596\n",
      "[50]\tvalid_0's multi_error: 0.182825\n",
      "[55]\tvalid_0's multi_error: 0.182825\n",
      "[60]\tvalid_0's multi_error: 0.180055\n",
      "[65]\tvalid_0's multi_error: 0.180055\n",
      "[70]\tvalid_0's multi_error: 0.180055\n",
      "[75]\tvalid_0's multi_error: 0.177285\n",
      "[80]\tvalid_0's multi_error: 0.177285\n",
      "[85]\tvalid_0's multi_error: 0.177285\n",
      "[90]\tvalid_0's multi_error: 0.177285\n",
      "[95]\tvalid_0's multi_error: 0.177285\n",
      "[100]\tvalid_0's multi_error: 0.177285\n",
      "[105]\tvalid_0's multi_error: 0.177285\n",
      "[110]\tvalid_0's multi_error: 0.177285\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's multi_error: 0.157895\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.105263\n",
      "[10]\tvalid_0's multi_error: 0.102493\n",
      "[15]\tvalid_0's multi_error: 0.0969529\n",
      "[20]\tvalid_0's multi_error: 0.0914127\n",
      "[25]\tvalid_0's multi_error: 0.0886427\n",
      "[30]\tvalid_0's multi_error: 0.0886427\n",
      "[35]\tvalid_0's multi_error: 0.0914127\n",
      "[40]\tvalid_0's multi_error: 0.0886427\n",
      "[45]\tvalid_0's multi_error: 0.0831025\n",
      "[50]\tvalid_0's multi_error: 0.0831025\n",
      "[55]\tvalid_0's multi_error: 0.0831025\n",
      "[60]\tvalid_0's multi_error: 0.0831025\n",
      "[65]\tvalid_0's multi_error: 0.0831025\n",
      "[70]\tvalid_0's multi_error: 0.0831025\n",
      "[75]\tvalid_0's multi_error: 0.0831025\n",
      "[80]\tvalid_0's multi_error: 0.0831025\n",
      "[85]\tvalid_0's multi_error: 0.0831025\n",
      "[90]\tvalid_0's multi_error: 0.0831025\n",
      "[95]\tvalid_0's multi_error: 0.0831025\n",
      "[100]\tvalid_0's multi_error: 0.0831025\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_error: 0.0803324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.138504\n",
      "[10]\tvalid_0's multi_error: 0.163435\n",
      "[15]\tvalid_0's multi_error: 0.163435\n",
      "[20]\tvalid_0's multi_error: 0.160665\n",
      "[25]\tvalid_0's multi_error: 0.160665\n",
      "[30]\tvalid_0's multi_error: 0.163435\n",
      "[35]\tvalid_0's multi_error: 0.160665\n",
      "[40]\tvalid_0's multi_error: 0.157895\n",
      "[45]\tvalid_0's multi_error: 0.157895\n",
      "[50]\tvalid_0's multi_error: 0.155125\n",
      "[55]\tvalid_0's multi_error: 0.160665\n",
      "[60]\tvalid_0's multi_error: 0.157895\n",
      "[65]\tvalid_0's multi_error: 0.157895\n",
      "[70]\tvalid_0's multi_error: 0.157895\n",
      "[75]\tvalid_0's multi_error: 0.157895\n",
      "[80]\tvalid_0's multi_error: 0.157895\n",
      "[85]\tvalid_0's multi_error: 0.160665\n",
      "[90]\tvalid_0's multi_error: 0.160665\n",
      "[95]\tvalid_0's multi_error: 0.160665\n",
      "[100]\tvalid_0's multi_error: 0.157895\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's multi_error: 0.113573\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.174515\n",
      "[10]\tvalid_0's multi_error: 0.210526\n",
      "[15]\tvalid_0's multi_error: 0.202216\n",
      "[20]\tvalid_0's multi_error: 0.202216\n",
      "[25]\tvalid_0's multi_error: 0.196676\n",
      "[30]\tvalid_0's multi_error: 0.199446\n",
      "[35]\tvalid_0's multi_error: 0.193906\n",
      "[40]\tvalid_0's multi_error: 0.191136\n",
      "[45]\tvalid_0's multi_error: 0.188366\n",
      "[50]\tvalid_0's multi_error: 0.193906\n",
      "[55]\tvalid_0's multi_error: 0.193906\n",
      "[60]\tvalid_0's multi_error: 0.193906\n",
      "[65]\tvalid_0's multi_error: 0.193906\n",
      "[70]\tvalid_0's multi_error: 0.193906\n",
      "[75]\tvalid_0's multi_error: 0.193906\n",
      "[80]\tvalid_0's multi_error: 0.193906\n",
      "[85]\tvalid_0's multi_error: 0.193906\n",
      "[90]\tvalid_0's multi_error: 0.193906\n",
      "[95]\tvalid_0's multi_error: 0.193906\n",
      "[100]\tvalid_0's multi_error: 0.193906\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's multi_error: 0.166205\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.0941828\n",
      "[10]\tvalid_0's multi_error: 0.108033\n",
      "[15]\tvalid_0's multi_error: 0.0914127\n",
      "[20]\tvalid_0's multi_error: 0.099723\n",
      "[25]\tvalid_0's multi_error: 0.099723\n",
      "[30]\tvalid_0's multi_error: 0.105263\n",
      "[35]\tvalid_0's multi_error: 0.102493\n",
      "[40]\tvalid_0's multi_error: 0.099723\n",
      "[45]\tvalid_0's multi_error: 0.102493\n",
      "[50]\tvalid_0's multi_error: 0.102493\n",
      "[55]\tvalid_0's multi_error: 0.105263\n",
      "[60]\tvalid_0's multi_error: 0.102493\n",
      "[65]\tvalid_0's multi_error: 0.102493\n",
      "[70]\tvalid_0's multi_error: 0.102493\n",
      "[75]\tvalid_0's multi_error: 0.0969529\n",
      "[80]\tvalid_0's multi_error: 0.0969529\n",
      "[85]\tvalid_0's multi_error: 0.0969529\n",
      "[90]\tvalid_0's multi_error: 0.0969529\n",
      "[95]\tvalid_0's multi_error: 0.0969529\n",
      "[100]\tvalid_0's multi_error: 0.0969529\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's multi_error: 0.0747922\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.263158\n",
      "[10]\tvalid_0's multi_error: 0.285319\n",
      "[15]\tvalid_0's multi_error: 0.285319\n",
      "[20]\tvalid_0's multi_error: 0.285319\n",
      "[25]\tvalid_0's multi_error: 0.296399\n",
      "[30]\tvalid_0's multi_error: 0.290859\n",
      "[35]\tvalid_0's multi_error: 0.293629\n",
      "[40]\tvalid_0's multi_error: 0.293629\n",
      "[45]\tvalid_0's multi_error: 0.296399\n",
      "[50]\tvalid_0's multi_error: 0.299169\n",
      "[55]\tvalid_0's multi_error: 0.290859\n",
      "[60]\tvalid_0's multi_error: 0.293629\n",
      "[65]\tvalid_0's multi_error: 0.296399\n",
      "[70]\tvalid_0's multi_error: 0.296399\n",
      "[75]\tvalid_0's multi_error: 0.299169\n",
      "[80]\tvalid_0's multi_error: 0.293629\n",
      "[85]\tvalid_0's multi_error: 0.299169\n",
      "[90]\tvalid_0's multi_error: 0.293629\n",
      "[95]\tvalid_0's multi_error: 0.290859\n",
      "[100]\tvalid_0's multi_error: 0.290859\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_error: 0.191136\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.110803\n",
      "[10]\tvalid_0's multi_error: 0.116343\n",
      "[15]\tvalid_0's multi_error: 0.113573\n",
      "[20]\tvalid_0's multi_error: 0.110803\n",
      "[25]\tvalid_0's multi_error: 0.116343\n",
      "[30]\tvalid_0's multi_error: 0.110803\n",
      "[35]\tvalid_0's multi_error: 0.110803\n",
      "[40]\tvalid_0's multi_error: 0.110803\n",
      "[45]\tvalid_0's multi_error: 0.110803\n",
      "[50]\tvalid_0's multi_error: 0.113573\n",
      "[55]\tvalid_0's multi_error: 0.116343\n",
      "[60]\tvalid_0's multi_error: 0.116343\n",
      "[65]\tvalid_0's multi_error: 0.116343\n",
      "[70]\tvalid_0's multi_error: 0.116343\n",
      "[75]\tvalid_0's multi_error: 0.116343\n",
      "[80]\tvalid_0's multi_error: 0.116343\n",
      "[85]\tvalid_0's multi_error: 0.116343\n",
      "[90]\tvalid_0's multi_error: 0.116343\n",
      "[95]\tvalid_0's multi_error: 0.116343\n",
      "[100]\tvalid_0's multi_error: 0.116343\n",
      "[105]\tvalid_0's multi_error: 0.116343\n",
      "[110]\tvalid_0's multi_error: 0.116343\n",
      "[115]\tvalid_0's multi_error: 0.116343\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's multi_error: 0.108033\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.127424\n",
      "[10]\tvalid_0's multi_error: 0.138504\n",
      "[15]\tvalid_0's multi_error: 0.138504\n",
      "[20]\tvalid_0's multi_error: 0.141274\n",
      "[25]\tvalid_0's multi_error: 0.132964\n",
      "[30]\tvalid_0's multi_error: 0.135734\n",
      "[35]\tvalid_0's multi_error: 0.135734\n",
      "[40]\tvalid_0's multi_error: 0.132964\n",
      "[45]\tvalid_0's multi_error: 0.132964\n",
      "[50]\tvalid_0's multi_error: 0.135734\n",
      "[55]\tvalid_0's multi_error: 0.135734\n",
      "[60]\tvalid_0's multi_error: 0.132964\n",
      "[65]\tvalid_0's multi_error: 0.130194\n",
      "[70]\tvalid_0's multi_error: 0.127424\n",
      "[75]\tvalid_0's multi_error: 0.127424\n",
      "[80]\tvalid_0's multi_error: 0.127424\n",
      "[85]\tvalid_0's multi_error: 0.127424\n",
      "[90]\tvalid_0's multi_error: 0.127424\n",
      "[95]\tvalid_0's multi_error: 0.127424\n",
      "[100]\tvalid_0's multi_error: 0.127424\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_error: 0.110803\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.144044\n",
      "[10]\tvalid_0's multi_error: 0.144044\n",
      "[15]\tvalid_0's multi_error: 0.138504\n",
      "[20]\tvalid_0's multi_error: 0.132964\n",
      "[25]\tvalid_0's multi_error: 0.130194\n",
      "[30]\tvalid_0's multi_error: 0.138504\n",
      "[35]\tvalid_0's multi_error: 0.138504\n",
      "[40]\tvalid_0's multi_error: 0.135734\n",
      "[45]\tvalid_0's multi_error: 0.138504\n",
      "[50]\tvalid_0's multi_error: 0.132964\n",
      "[55]\tvalid_0's multi_error: 0.130194\n",
      "[60]\tvalid_0's multi_error: 0.132964\n",
      "[65]\tvalid_0's multi_error: 0.132964\n",
      "[70]\tvalid_0's multi_error: 0.132964\n",
      "[75]\tvalid_0's multi_error: 0.132964\n",
      "[80]\tvalid_0's multi_error: 0.132964\n",
      "[85]\tvalid_0's multi_error: 0.132964\n",
      "[90]\tvalid_0's multi_error: 0.132964\n",
      "[95]\tvalid_0's multi_error: 0.132964\n",
      "[100]\tvalid_0's multi_error: 0.132964\n",
      "[105]\tvalid_0's multi_error: 0.132964\n",
      "[110]\tvalid_0's multi_error: 0.132964\n",
      "[115]\tvalid_0's multi_error: 0.132964\n",
      "[120]\tvalid_0's multi_error: 0.132964\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's multi_error: 0.130194\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.130194\n",
      "[10]\tvalid_0's multi_error: 0.102493\n",
      "[15]\tvalid_0's multi_error: 0.102493\n",
      "[20]\tvalid_0's multi_error: 0.105263\n",
      "[25]\tvalid_0's multi_error: 0.102493\n",
      "[30]\tvalid_0's multi_error: 0.0941828\n",
      "[35]\tvalid_0's multi_error: 0.0886427\n",
      "[40]\tvalid_0's multi_error: 0.0886427\n",
      "[45]\tvalid_0's multi_error: 0.0886427\n",
      "[50]\tvalid_0's multi_error: 0.0914127\n",
      "[55]\tvalid_0's multi_error: 0.0914127\n",
      "[60]\tvalid_0's multi_error: 0.0886427\n",
      "[65]\tvalid_0's multi_error: 0.0914127\n",
      "[70]\tvalid_0's multi_error: 0.0886427\n",
      "[75]\tvalid_0's multi_error: 0.0886427\n",
      "[80]\tvalid_0's multi_error: 0.0914127\n",
      "[85]\tvalid_0's multi_error: 0.0886427\n",
      "[90]\tvalid_0's multi_error: 0.0886427\n",
      "[95]\tvalid_0's multi_error: 0.0886427\n",
      "[100]\tvalid_0's multi_error: 0.0914127\n",
      "[105]\tvalid_0's multi_error: 0.0886427\n",
      "[110]\tvalid_0's multi_error: 0.0886427\n",
      "[115]\tvalid_0's multi_error: 0.0886427\n",
      "[120]\tvalid_0's multi_error: 0.0914127\n",
      "[125]\tvalid_0's multi_error: 0.0886427\n",
      "[130]\tvalid_0's multi_error: 0.0886427\n",
      "[135]\tvalid_0's multi_error: 0.0886427\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's multi_error: 0.0858726\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.152355\n",
      "[10]\tvalid_0's multi_error: 0.141274\n",
      "[15]\tvalid_0's multi_error: 0.146814\n",
      "[20]\tvalid_0's multi_error: 0.144044\n",
      "[25]\tvalid_0's multi_error: 0.146814\n",
      "[30]\tvalid_0's multi_error: 0.135734\n",
      "[35]\tvalid_0's multi_error: 0.138504\n",
      "[40]\tvalid_0's multi_error: 0.138504\n",
      "[45]\tvalid_0's multi_error: 0.138504\n",
      "[50]\tvalid_0's multi_error: 0.141274\n",
      "[55]\tvalid_0's multi_error: 0.144044\n",
      "[60]\tvalid_0's multi_error: 0.152355\n",
      "[65]\tvalid_0's multi_error: 0.144044\n",
      "[70]\tvalid_0's multi_error: 0.146814\n",
      "[75]\tvalid_0's multi_error: 0.152355\n",
      "[80]\tvalid_0's multi_error: 0.149584\n",
      "[85]\tvalid_0's multi_error: 0.146814\n",
      "[90]\tvalid_0's multi_error: 0.144044\n",
      "[95]\tvalid_0's multi_error: 0.144044\n",
      "[100]\tvalid_0's multi_error: 0.146814\n",
      "[105]\tvalid_0's multi_error: 0.149584\n",
      "[110]\tvalid_0's multi_error: 0.146814\n",
      "[115]\tvalid_0's multi_error: 0.146814\n",
      "[120]\tvalid_0's multi_error: 0.146814\n",
      "[125]\tvalid_0's multi_error: 0.146814\n",
      "[130]\tvalid_0's multi_error: 0.149584\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's multi_error: 0.135734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.124654\n",
      "[10]\tvalid_0's multi_error: 0.127424\n",
      "[15]\tvalid_0's multi_error: 0.127424\n",
      "[20]\tvalid_0's multi_error: 0.113573\n",
      "[25]\tvalid_0's multi_error: 0.116343\n",
      "[30]\tvalid_0's multi_error: 0.116343\n",
      "[35]\tvalid_0's multi_error: 0.119114\n",
      "[40]\tvalid_0's multi_error: 0.116343\n",
      "[45]\tvalid_0's multi_error: 0.116343\n",
      "[50]\tvalid_0's multi_error: 0.110803\n",
      "[55]\tvalid_0's multi_error: 0.105263\n",
      "[60]\tvalid_0's multi_error: 0.110803\n",
      "[65]\tvalid_0's multi_error: 0.110803\n",
      "[70]\tvalid_0's multi_error: 0.110803\n",
      "[75]\tvalid_0's multi_error: 0.113573\n",
      "[80]\tvalid_0's multi_error: 0.110803\n",
      "[85]\tvalid_0's multi_error: 0.113573\n",
      "[90]\tvalid_0's multi_error: 0.110803\n",
      "[95]\tvalid_0's multi_error: 0.108033\n",
      "[100]\tvalid_0's multi_error: 0.110803\n",
      "[105]\tvalid_0's multi_error: 0.110803\n",
      "[110]\tvalid_0's multi_error: 0.113573\n",
      "[115]\tvalid_0's multi_error: 0.110803\n",
      "[120]\tvalid_0's multi_error: 0.110803\n",
      "[125]\tvalid_0's multi_error: 0.113573\n",
      "[130]\tvalid_0's multi_error: 0.110803\n",
      "[135]\tvalid_0's multi_error: 0.110803\n",
      "[140]\tvalid_0's multi_error: 0.110803\n",
      "[145]\tvalid_0's multi_error: 0.110803\n",
      "[150]\tvalid_0's multi_error: 0.110803\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's multi_error: 0.105263\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.515235\n",
      "[10]\tvalid_0's multi_error: 0.379501\n",
      "[15]\tvalid_0's multi_error: 0.265928\n",
      "[20]\tvalid_0's multi_error: 0.260388\n",
      "[25]\tvalid_0's multi_error: 0.249307\n",
      "[30]\tvalid_0's multi_error: 0.265928\n",
      "[35]\tvalid_0's multi_error: 0.282548\n",
      "[40]\tvalid_0's multi_error: 0.279778\n",
      "[45]\tvalid_0's multi_error: 0.271468\n",
      "[50]\tvalid_0's multi_error: 0.279778\n",
      "[55]\tvalid_0's multi_error: 0.282548\n",
      "[60]\tvalid_0's multi_error: 0.279778\n",
      "[65]\tvalid_0's multi_error: 0.279778\n",
      "[70]\tvalid_0's multi_error: 0.279778\n",
      "[75]\tvalid_0's multi_error: 0.279778\n",
      "[80]\tvalid_0's multi_error: 0.271468\n",
      "[85]\tvalid_0's multi_error: 0.268698\n",
      "[90]\tvalid_0's multi_error: 0.271468\n",
      "[95]\tvalid_0's multi_error: 0.265928\n",
      "[100]\tvalid_0's multi_error: 0.268698\n",
      "[105]\tvalid_0's multi_error: 0.268698\n",
      "[110]\tvalid_0's multi_error: 0.268698\n",
      "[115]\tvalid_0's multi_error: 0.268698\n",
      "[120]\tvalid_0's multi_error: 0.268698\n",
      "[125]\tvalid_0's multi_error: 0.271468\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's multi_error: 0.243767\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.506925\n",
      "[10]\tvalid_0's multi_error: 0.387812\n",
      "[15]\tvalid_0's multi_error: 0.307479\n",
      "[20]\tvalid_0's multi_error: 0.279778\n",
      "[25]\tvalid_0's multi_error: 0.271468\n",
      "[30]\tvalid_0's multi_error: 0.282548\n",
      "[35]\tvalid_0's multi_error: 0.282548\n",
      "[40]\tvalid_0's multi_error: 0.282548\n",
      "[45]\tvalid_0's multi_error: 0.299169\n",
      "[50]\tvalid_0's multi_error: 0.307479\n",
      "[55]\tvalid_0's multi_error: 0.301939\n",
      "[60]\tvalid_0's multi_error: 0.301939\n",
      "[65]\tvalid_0's multi_error: 0.296399\n",
      "[70]\tvalid_0's multi_error: 0.299169\n",
      "[75]\tvalid_0's multi_error: 0.285319\n",
      "[80]\tvalid_0's multi_error: 0.293629\n",
      "[85]\tvalid_0's multi_error: 0.288089\n",
      "[90]\tvalid_0's multi_error: 0.288089\n",
      "[95]\tvalid_0's multi_error: 0.290859\n",
      "[100]\tvalid_0's multi_error: 0.288089\n",
      "[105]\tvalid_0's multi_error: 0.285319\n",
      "[110]\tvalid_0's multi_error: 0.288089\n",
      "[115]\tvalid_0's multi_error: 0.285319\n",
      "[120]\tvalid_0's multi_error: 0.279778\n",
      "[125]\tvalid_0's multi_error: 0.277008\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's multi_error: 0.252078\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.171745\n",
      "[10]\tvalid_0's multi_error: 0.152355\n",
      "[15]\tvalid_0's multi_error: 0.152355\n",
      "[20]\tvalid_0's multi_error: 0.138504\n",
      "[25]\tvalid_0's multi_error: 0.144044\n",
      "[30]\tvalid_0's multi_error: 0.138504\n",
      "[35]\tvalid_0's multi_error: 0.124654\n",
      "[40]\tvalid_0's multi_error: 0.132964\n",
      "[45]\tvalid_0's multi_error: 0.124654\n",
      "[50]\tvalid_0's multi_error: 0.130194\n",
      "[55]\tvalid_0's multi_error: 0.138504\n",
      "[60]\tvalid_0's multi_error: 0.132964\n",
      "[65]\tvalid_0's multi_error: 0.135734\n",
      "[70]\tvalid_0's multi_error: 0.135734\n",
      "[75]\tvalid_0's multi_error: 0.135734\n",
      "[80]\tvalid_0's multi_error: 0.138504\n",
      "[85]\tvalid_0's multi_error: 0.138504\n",
      "[90]\tvalid_0's multi_error: 0.141274\n",
      "[95]\tvalid_0's multi_error: 0.138504\n",
      "[100]\tvalid_0's multi_error: 0.138504\n",
      "[105]\tvalid_0's multi_error: 0.138504\n",
      "[110]\tvalid_0's multi_error: 0.138504\n",
      "[115]\tvalid_0's multi_error: 0.135734\n",
      "[120]\tvalid_0's multi_error: 0.135734\n",
      "[125]\tvalid_0's multi_error: 0.132964\n",
      "[130]\tvalid_0's multi_error: 0.132964\n",
      "[135]\tvalid_0's multi_error: 0.130194\n",
      "[140]\tvalid_0's multi_error: 0.132964\n",
      "[145]\tvalid_0's multi_error: 0.135734\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's multi_error: 0.119114\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.0914127\n",
      "[10]\tvalid_0's multi_error: 0.0775623\n",
      "[15]\tvalid_0's multi_error: 0.0720222\n",
      "[20]\tvalid_0's multi_error: 0.0692521\n",
      "[25]\tvalid_0's multi_error: 0.0554017\n",
      "[30]\tvalid_0's multi_error: 0.0692521\n",
      "[35]\tvalid_0's multi_error: 0.0720222\n",
      "[40]\tvalid_0's multi_error: 0.0720222\n",
      "[45]\tvalid_0's multi_error: 0.0637119\n",
      "[50]\tvalid_0's multi_error: 0.0720222\n",
      "[55]\tvalid_0's multi_error: 0.0747922\n",
      "[60]\tvalid_0's multi_error: 0.0803324\n",
      "[65]\tvalid_0's multi_error: 0.0803324\n",
      "[70]\tvalid_0's multi_error: 0.0803324\n",
      "[75]\tvalid_0's multi_error: 0.0775623\n",
      "[80]\tvalid_0's multi_error: 0.0747922\n",
      "[85]\tvalid_0's multi_error: 0.0747922\n",
      "[90]\tvalid_0's multi_error: 0.0747922\n",
      "[95]\tvalid_0's multi_error: 0.0747922\n",
      "[100]\tvalid_0's multi_error: 0.0747922\n",
      "[105]\tvalid_0's multi_error: 0.0747922\n",
      "[110]\tvalid_0's multi_error: 0.0747922\n",
      "[115]\tvalid_0's multi_error: 0.0747922\n",
      "[120]\tvalid_0's multi_error: 0.0747922\n",
      "[125]\tvalid_0's multi_error: 0.0747922\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's multi_error: 0.0554017\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.229917\n",
      "[10]\tvalid_0's multi_error: 0.221607\n",
      "[15]\tvalid_0's multi_error: 0.202216\n",
      "[20]\tvalid_0's multi_error: 0.216066\n",
      "[25]\tvalid_0's multi_error: 0.210526\n",
      "[30]\tvalid_0's multi_error: 0.213296\n",
      "[35]\tvalid_0's multi_error: 0.216066\n",
      "[40]\tvalid_0's multi_error: 0.204986\n",
      "[45]\tvalid_0's multi_error: 0.193906\n",
      "[50]\tvalid_0's multi_error: 0.199446\n",
      "[55]\tvalid_0's multi_error: 0.199446\n",
      "[60]\tvalid_0's multi_error: 0.202216\n",
      "[65]\tvalid_0's multi_error: 0.199446\n",
      "[70]\tvalid_0's multi_error: 0.196676\n",
      "[75]\tvalid_0's multi_error: 0.188366\n",
      "[80]\tvalid_0's multi_error: 0.188366\n",
      "[85]\tvalid_0's multi_error: 0.188366\n",
      "[90]\tvalid_0's multi_error: 0.191136\n",
      "[95]\tvalid_0's multi_error: 0.191136\n",
      "[100]\tvalid_0's multi_error: 0.193906\n",
      "[105]\tvalid_0's multi_error: 0.193906\n",
      "[110]\tvalid_0's multi_error: 0.193906\n",
      "[115]\tvalid_0's multi_error: 0.196676\n",
      "[120]\tvalid_0's multi_error: 0.196676\n",
      "[125]\tvalid_0's multi_error: 0.196676\n",
      "[130]\tvalid_0's multi_error: 0.196676\n",
      "[135]\tvalid_0's multi_error: 0.196676\n",
      "[140]\tvalid_0's multi_error: 0.196676\n",
      "[145]\tvalid_0's multi_error: 0.196676\n",
      "[150]\tvalid_0's multi_error: 0.196676\n",
      "[155]\tvalid_0's multi_error: 0.196676\n",
      "[160]\tvalid_0's multi_error: 0.196676\n",
      "[165]\tvalid_0's multi_error: 0.196676\n",
      "[170]\tvalid_0's multi_error: 0.196676\n",
      "[175]\tvalid_0's multi_error: 0.196676\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's multi_error: 0.185596\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.138504\n",
      "[10]\tvalid_0's multi_error: 0.127424\n",
      "[15]\tvalid_0's multi_error: 0.119114\n",
      "[20]\tvalid_0's multi_error: 0.124654\n",
      "[25]\tvalid_0's multi_error: 0.113573\n",
      "[30]\tvalid_0's multi_error: 0.110803\n",
      "[35]\tvalid_0's multi_error: 0.113573\n",
      "[40]\tvalid_0's multi_error: 0.113573\n",
      "[45]\tvalid_0's multi_error: 0.105263\n",
      "[50]\tvalid_0's multi_error: 0.113573\n",
      "[55]\tvalid_0's multi_error: 0.110803\n",
      "[60]\tvalid_0's multi_error: 0.110803\n",
      "[65]\tvalid_0's multi_error: 0.110803\n",
      "[70]\tvalid_0's multi_error: 0.108033\n",
      "[75]\tvalid_0's multi_error: 0.110803\n",
      "[80]\tvalid_0's multi_error: 0.110803\n",
      "[85]\tvalid_0's multi_error: 0.110803\n",
      "[90]\tvalid_0's multi_error: 0.110803\n",
      "[95]\tvalid_0's multi_error: 0.110803\n",
      "[100]\tvalid_0's multi_error: 0.110803\n",
      "[105]\tvalid_0's multi_error: 0.110803\n",
      "[110]\tvalid_0's multi_error: 0.110803\n",
      "[115]\tvalid_0's multi_error: 0.110803\n",
      "[120]\tvalid_0's multi_error: 0.110803\n",
      "[125]\tvalid_0's multi_error: 0.110803\n",
      "[130]\tvalid_0's multi_error: 0.110803\n",
      "[135]\tvalid_0's multi_error: 0.108033\n",
      "[140]\tvalid_0's multi_error: 0.108033\n",
      "[145]\tvalid_0's multi_error: 0.110803\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's multi_error: 0.105263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.246537\n",
      "[10]\tvalid_0's multi_error: 0.238227\n",
      "[15]\tvalid_0's multi_error: 0.232687\n",
      "[20]\tvalid_0's multi_error: 0.238227\n",
      "[25]\tvalid_0's multi_error: 0.232687\n",
      "[30]\tvalid_0's multi_error: 0.229917\n",
      "[35]\tvalid_0's multi_error: 0.235457\n",
      "[40]\tvalid_0's multi_error: 0.229917\n",
      "[45]\tvalid_0's multi_error: 0.227147\n",
      "[50]\tvalid_0's multi_error: 0.229917\n",
      "[55]\tvalid_0's multi_error: 0.227147\n",
      "[60]\tvalid_0's multi_error: 0.227147\n",
      "[65]\tvalid_0's multi_error: 0.221607\n",
      "[70]\tvalid_0's multi_error: 0.221607\n",
      "[75]\tvalid_0's multi_error: 0.221607\n",
      "[80]\tvalid_0's multi_error: 0.221607\n",
      "[85]\tvalid_0's multi_error: 0.221607\n",
      "[90]\tvalid_0's multi_error: 0.221607\n",
      "[95]\tvalid_0's multi_error: 0.221607\n",
      "[100]\tvalid_0's multi_error: 0.221607\n",
      "[105]\tvalid_0's multi_error: 0.221607\n",
      "[110]\tvalid_0's multi_error: 0.221607\n",
      "[115]\tvalid_0's multi_error: 0.221607\n",
      "[120]\tvalid_0's multi_error: 0.221607\n",
      "[125]\tvalid_0's multi_error: 0.221607\n",
      "[130]\tvalid_0's multi_error: 0.221607\n",
      "[135]\tvalid_0's multi_error: 0.221607\n",
      "[140]\tvalid_0's multi_error: 0.221607\n",
      "[145]\tvalid_0's multi_error: 0.221607\n",
      "[150]\tvalid_0's multi_error: 0.221607\n",
      "[155]\tvalid_0's multi_error: 0.221607\n",
      "[160]\tvalid_0's multi_error: 0.221607\n",
      "[165]\tvalid_0's multi_error: 0.221607\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's multi_error: 0.221607\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.188366\n",
      "[10]\tvalid_0's multi_error: 0.199446\n",
      "[15]\tvalid_0's multi_error: 0.191136\n",
      "[20]\tvalid_0's multi_error: 0.196676\n",
      "[25]\tvalid_0's multi_error: 0.204986\n",
      "[30]\tvalid_0's multi_error: 0.207756\n",
      "[35]\tvalid_0's multi_error: 0.207756\n",
      "[40]\tvalid_0's multi_error: 0.207756\n",
      "[45]\tvalid_0's multi_error: 0.207756\n",
      "[50]\tvalid_0's multi_error: 0.202216\n",
      "[55]\tvalid_0's multi_error: 0.202216\n",
      "[60]\tvalid_0's multi_error: 0.202216\n",
      "[65]\tvalid_0's multi_error: 0.199446\n",
      "[70]\tvalid_0's multi_error: 0.204986\n",
      "[75]\tvalid_0's multi_error: 0.204986\n",
      "[80]\tvalid_0's multi_error: 0.204986\n",
      "[85]\tvalid_0's multi_error: 0.204986\n",
      "[90]\tvalid_0's multi_error: 0.204986\n",
      "[95]\tvalid_0's multi_error: 0.204986\n",
      "[100]\tvalid_0's multi_error: 0.204986\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's multi_error: 0.185596\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.33241\n",
      "[10]\tvalid_0's multi_error: 0.313019\n",
      "[15]\tvalid_0's multi_error: 0.299169\n",
      "[20]\tvalid_0's multi_error: 0.313019\n",
      "[25]\tvalid_0's multi_error: 0.301939\n",
      "[30]\tvalid_0's multi_error: 0.304709\n",
      "[35]\tvalid_0's multi_error: 0.304709\n",
      "[40]\tvalid_0's multi_error: 0.299169\n",
      "[45]\tvalid_0's multi_error: 0.304709\n",
      "[50]\tvalid_0's multi_error: 0.313019\n",
      "[55]\tvalid_0's multi_error: 0.32133\n",
      "[60]\tvalid_0's multi_error: 0.315789\n",
      "[65]\tvalid_0's multi_error: 0.315789\n",
      "[70]\tvalid_0's multi_error: 0.315789\n",
      "[75]\tvalid_0's multi_error: 0.315789\n",
      "[80]\tvalid_0's multi_error: 0.315789\n",
      "[85]\tvalid_0's multi_error: 0.31856\n",
      "[90]\tvalid_0's multi_error: 0.31856\n",
      "[95]\tvalid_0's multi_error: 0.315789\n",
      "[100]\tvalid_0's multi_error: 0.31856\n",
      "[105]\tvalid_0's multi_error: 0.31856\n",
      "[110]\tvalid_0's multi_error: 0.31856\n",
      "[115]\tvalid_0's multi_error: 0.31856\n",
      "[120]\tvalid_0's multi_error: 0.31856\n",
      "[125]\tvalid_0's multi_error: 0.31856\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's multi_error: 0.288089\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.177285\n",
      "[10]\tvalid_0's multi_error: 0.144044\n",
      "[15]\tvalid_0's multi_error: 0.138504\n",
      "[20]\tvalid_0's multi_error: 0.130194\n",
      "[25]\tvalid_0's multi_error: 0.124654\n",
      "[30]\tvalid_0's multi_error: 0.116343\n",
      "[35]\tvalid_0's multi_error: 0.116343\n",
      "[40]\tvalid_0's multi_error: 0.121884\n",
      "[45]\tvalid_0's multi_error: 0.124654\n",
      "[50]\tvalid_0's multi_error: 0.124654\n",
      "[55]\tvalid_0's multi_error: 0.119114\n",
      "[60]\tvalid_0's multi_error: 0.124654\n",
      "[65]\tvalid_0's multi_error: 0.121884\n",
      "[70]\tvalid_0's multi_error: 0.124654\n",
      "[75]\tvalid_0's multi_error: 0.124654\n",
      "[80]\tvalid_0's multi_error: 0.121884\n",
      "[85]\tvalid_0's multi_error: 0.124654\n",
      "[90]\tvalid_0's multi_error: 0.124654\n",
      "[95]\tvalid_0's multi_error: 0.116343\n",
      "[100]\tvalid_0's multi_error: 0.119114\n",
      "[105]\tvalid_0's multi_error: 0.116343\n",
      "[110]\tvalid_0's multi_error: 0.119114\n",
      "[115]\tvalid_0's multi_error: 0.116343\n",
      "[120]\tvalid_0's multi_error: 0.116343\n",
      "[125]\tvalid_0's multi_error: 0.116343\n",
      "[130]\tvalid_0's multi_error: 0.116343\n",
      "[135]\tvalid_0's multi_error: 0.119114\n",
      "[140]\tvalid_0's multi_error: 0.119114\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's multi_error: 0.113573\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.277008\n",
      "[10]\tvalid_0's multi_error: 0.282548\n",
      "[15]\tvalid_0's multi_error: 0.271468\n",
      "[20]\tvalid_0's multi_error: 0.268698\n",
      "[25]\tvalid_0's multi_error: 0.257618\n",
      "[30]\tvalid_0's multi_error: 0.263158\n",
      "[35]\tvalid_0's multi_error: 0.265928\n",
      "[40]\tvalid_0's multi_error: 0.260388\n",
      "[45]\tvalid_0's multi_error: 0.257618\n",
      "[50]\tvalid_0's multi_error: 0.257618\n",
      "[55]\tvalid_0's multi_error: 0.254848\n",
      "[60]\tvalid_0's multi_error: 0.257618\n",
      "[65]\tvalid_0's multi_error: 0.257618\n",
      "[70]\tvalid_0's multi_error: 0.260388\n",
      "[75]\tvalid_0's multi_error: 0.257618\n",
      "[80]\tvalid_0's multi_error: 0.257618\n",
      "[85]\tvalid_0's multi_error: 0.254848\n",
      "[90]\tvalid_0's multi_error: 0.254848\n",
      "[95]\tvalid_0's multi_error: 0.254848\n",
      "[100]\tvalid_0's multi_error: 0.254848\n",
      "[105]\tvalid_0's multi_error: 0.254848\n",
      "[110]\tvalid_0's multi_error: 0.254848\n",
      "[115]\tvalid_0's multi_error: 0.252078\n",
      "[120]\tvalid_0's multi_error: 0.254848\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's multi_error: 0.252078\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.124654\n",
      "[10]\tvalid_0's multi_error: 0.119114\n",
      "[15]\tvalid_0's multi_error: 0.127424\n",
      "[20]\tvalid_0's multi_error: 0.121884\n",
      "[25]\tvalid_0's multi_error: 0.119114\n",
      "[30]\tvalid_0's multi_error: 0.124654\n",
      "[35]\tvalid_0's multi_error: 0.121884\n",
      "[40]\tvalid_0's multi_error: 0.116343\n",
      "[45]\tvalid_0's multi_error: 0.113573\n",
      "[50]\tvalid_0's multi_error: 0.105263\n",
      "[55]\tvalid_0's multi_error: 0.105263\n",
      "[60]\tvalid_0's multi_error: 0.105263\n",
      "[65]\tvalid_0's multi_error: 0.0969529\n",
      "[70]\tvalid_0's multi_error: 0.0941828\n",
      "[75]\tvalid_0's multi_error: 0.0969529\n",
      "[80]\tvalid_0's multi_error: 0.0969529\n",
      "[85]\tvalid_0's multi_error: 0.099723\n",
      "[90]\tvalid_0's multi_error: 0.099723\n",
      "[95]\tvalid_0's multi_error: 0.102493\n",
      "[100]\tvalid_0's multi_error: 0.102493\n",
      "[105]\tvalid_0's multi_error: 0.102493\n",
      "[110]\tvalid_0's multi_error: 0.102493\n",
      "[115]\tvalid_0's multi_error: 0.102493\n",
      "[120]\tvalid_0's multi_error: 0.102493\n",
      "[125]\tvalid_0's multi_error: 0.105263\n",
      "[130]\tvalid_0's multi_error: 0.102493\n",
      "[135]\tvalid_0's multi_error: 0.102493\n",
      "[140]\tvalid_0's multi_error: 0.105263\n",
      "[145]\tvalid_0's multi_error: 0.105263\n",
      "[150]\tvalid_0's multi_error: 0.102493\n",
      "[155]\tvalid_0's multi_error: 0.105263\n",
      "[160]\tvalid_0's multi_error: 0.105263\n",
      "[165]\tvalid_0's multi_error: 0.105263\n",
      "[170]\tvalid_0's multi_error: 0.102493\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's multi_error: 0.0941828\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.196676\n",
      "[10]\tvalid_0's multi_error: 0.207756\n",
      "[15]\tvalid_0's multi_error: 0.182825\n",
      "[20]\tvalid_0's multi_error: 0.177285\n",
      "[25]\tvalid_0's multi_error: 0.191136\n",
      "[30]\tvalid_0's multi_error: 0.182825\n",
      "[35]\tvalid_0's multi_error: 0.188366\n",
      "[40]\tvalid_0's multi_error: 0.191136\n",
      "[45]\tvalid_0's multi_error: 0.199446\n",
      "[50]\tvalid_0's multi_error: 0.193906\n",
      "[55]\tvalid_0's multi_error: 0.204986\n",
      "[60]\tvalid_0's multi_error: 0.204986\n",
      "[65]\tvalid_0's multi_error: 0.202216\n",
      "[70]\tvalid_0's multi_error: 0.204986\n",
      "[75]\tvalid_0's multi_error: 0.204986\n",
      "[80]\tvalid_0's multi_error: 0.202216\n",
      "[85]\tvalid_0's multi_error: 0.199446\n",
      "[90]\tvalid_0's multi_error: 0.199446\n",
      "[95]\tvalid_0's multi_error: 0.199446\n",
      "[100]\tvalid_0's multi_error: 0.199446\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_error: 0.149584\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.216066\n",
      "[10]\tvalid_0's multi_error: 0.218837\n",
      "[15]\tvalid_0's multi_error: 0.202216\n",
      "[20]\tvalid_0's multi_error: 0.185596\n",
      "[25]\tvalid_0's multi_error: 0.202216\n",
      "[30]\tvalid_0's multi_error: 0.191136\n",
      "[35]\tvalid_0's multi_error: 0.199446\n",
      "[40]\tvalid_0's multi_error: 0.202216\n",
      "[45]\tvalid_0's multi_error: 0.202216\n",
      "[50]\tvalid_0's multi_error: 0.199446\n",
      "[55]\tvalid_0's multi_error: 0.202216\n",
      "[60]\tvalid_0's multi_error: 0.202216\n",
      "[65]\tvalid_0's multi_error: 0.193906\n",
      "[70]\tvalid_0's multi_error: 0.193906\n",
      "[75]\tvalid_0's multi_error: 0.196676\n",
      "[80]\tvalid_0's multi_error: 0.199446\n",
      "[85]\tvalid_0's multi_error: 0.196676\n",
      "[90]\tvalid_0's multi_error: 0.196676\n",
      "[95]\tvalid_0's multi_error: 0.199446\n",
      "[100]\tvalid_0's multi_error: 0.199446\n",
      "[105]\tvalid_0's multi_error: 0.204986\n",
      "[110]\tvalid_0's multi_error: 0.202216\n",
      "[115]\tvalid_0's multi_error: 0.199446\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_error: 0.185596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.132964\n",
      "[10]\tvalid_0's multi_error: 0.121884\n",
      "[15]\tvalid_0's multi_error: 0.121884\n",
      "[20]\tvalid_0's multi_error: 0.127424\n",
      "[25]\tvalid_0's multi_error: 0.113573\n",
      "[30]\tvalid_0's multi_error: 0.119114\n",
      "[35]\tvalid_0's multi_error: 0.121884\n",
      "[40]\tvalid_0's multi_error: 0.116343\n",
      "[45]\tvalid_0's multi_error: 0.110803\n",
      "[50]\tvalid_0's multi_error: 0.113573\n",
      "[55]\tvalid_0's multi_error: 0.119114\n",
      "[60]\tvalid_0's multi_error: 0.113573\n",
      "[65]\tvalid_0's multi_error: 0.113573\n",
      "[70]\tvalid_0's multi_error: 0.110803\n",
      "[75]\tvalid_0's multi_error: 0.110803\n",
      "[80]\tvalid_0's multi_error: 0.110803\n",
      "[85]\tvalid_0's multi_error: 0.110803\n",
      "[90]\tvalid_0's multi_error: 0.113573\n",
      "[95]\tvalid_0's multi_error: 0.113573\n",
      "[100]\tvalid_0's multi_error: 0.113573\n",
      "[105]\tvalid_0's multi_error: 0.113573\n",
      "[110]\tvalid_0's multi_error: 0.116343\n",
      "[115]\tvalid_0's multi_error: 0.113573\n",
      "[120]\tvalid_0's multi_error: 0.113573\n",
      "[125]\tvalid_0's multi_error: 0.113573\n",
      "[130]\tvalid_0's multi_error: 0.113573\n",
      "[135]\tvalid_0's multi_error: 0.113573\n",
      "[140]\tvalid_0's multi_error: 0.113573\n",
      "[145]\tvalid_0's multi_error: 0.113573\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's multi_error: 0.108033\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.110803\n",
      "[10]\tvalid_0's multi_error: 0.0858726\n",
      "[15]\tvalid_0's multi_error: 0.0858726\n",
      "[20]\tvalid_0's multi_error: 0.0803324\n",
      "[25]\tvalid_0's multi_error: 0.0886427\n",
      "[30]\tvalid_0's multi_error: 0.0941828\n",
      "[35]\tvalid_0's multi_error: 0.0969529\n",
      "[40]\tvalid_0's multi_error: 0.0941828\n",
      "[45]\tvalid_0's multi_error: 0.0941828\n",
      "[50]\tvalid_0's multi_error: 0.0941828\n",
      "[55]\tvalid_0's multi_error: 0.099723\n",
      "[60]\tvalid_0's multi_error: 0.0969529\n",
      "[65]\tvalid_0's multi_error: 0.0941828\n",
      "[70]\tvalid_0's multi_error: 0.0941828\n",
      "[75]\tvalid_0's multi_error: 0.099723\n",
      "[80]\tvalid_0's multi_error: 0.0969529\n",
      "[85]\tvalid_0's multi_error: 0.0969529\n",
      "[90]\tvalid_0's multi_error: 0.0969529\n",
      "[95]\tvalid_0's multi_error: 0.0969529\n",
      "[100]\tvalid_0's multi_error: 0.0969529\n",
      "[105]\tvalid_0's multi_error: 0.0969529\n",
      "[110]\tvalid_0's multi_error: 0.0969529\n",
      "[115]\tvalid_0's multi_error: 0.0969529\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's multi_error: 0.0775623\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.232687\n",
      "[10]\tvalid_0's multi_error: 0.229917\n",
      "[15]\tvalid_0's multi_error: 0.229917\n",
      "[20]\tvalid_0's multi_error: 0.229917\n",
      "[25]\tvalid_0's multi_error: 0.229917\n",
      "[30]\tvalid_0's multi_error: 0.232687\n",
      "[35]\tvalid_0's multi_error: 0.229917\n",
      "[40]\tvalid_0's multi_error: 0.227147\n",
      "[45]\tvalid_0's multi_error: 0.224377\n",
      "[50]\tvalid_0's multi_error: 0.227147\n",
      "[55]\tvalid_0's multi_error: 0.221607\n",
      "[60]\tvalid_0's multi_error: 0.221607\n",
      "[65]\tvalid_0's multi_error: 0.221607\n",
      "[70]\tvalid_0's multi_error: 0.221607\n",
      "[75]\tvalid_0's multi_error: 0.221607\n",
      "[80]\tvalid_0's multi_error: 0.218837\n",
      "[85]\tvalid_0's multi_error: 0.218837\n",
      "[90]\tvalid_0's multi_error: 0.218837\n",
      "[95]\tvalid_0's multi_error: 0.218837\n",
      "[100]\tvalid_0's multi_error: 0.218837\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_error: 0.188366\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.171745\n",
      "[10]\tvalid_0's multi_error: 0.155125\n",
      "[15]\tvalid_0's multi_error: 0.163435\n",
      "[20]\tvalid_0's multi_error: 0.144044\n",
      "[25]\tvalid_0's multi_error: 0.132964\n",
      "[30]\tvalid_0's multi_error: 0.144044\n",
      "[35]\tvalid_0's multi_error: 0.141274\n",
      "[40]\tvalid_0's multi_error: 0.135734\n",
      "[45]\tvalid_0's multi_error: 0.130194\n",
      "[50]\tvalid_0's multi_error: 0.135734\n",
      "[55]\tvalid_0's multi_error: 0.135734\n",
      "[60]\tvalid_0's multi_error: 0.132964\n",
      "[65]\tvalid_0's multi_error: 0.132964\n",
      "[70]\tvalid_0's multi_error: 0.132964\n",
      "[75]\tvalid_0's multi_error: 0.135734\n",
      "[80]\tvalid_0's multi_error: 0.132964\n",
      "[85]\tvalid_0's multi_error: 0.132964\n",
      "[90]\tvalid_0's multi_error: 0.132964\n",
      "[95]\tvalid_0's multi_error: 0.132964\n",
      "[100]\tvalid_0's multi_error: 0.132964\n",
      "[105]\tvalid_0's multi_error: 0.132964\n",
      "[110]\tvalid_0's multi_error: 0.132964\n",
      "[115]\tvalid_0's multi_error: 0.132964\n",
      "[120]\tvalid_0's multi_error: 0.135734\n",
      "[125]\tvalid_0's multi_error: 0.135734\n",
      "[130]\tvalid_0's multi_error: 0.132964\n",
      "[135]\tvalid_0's multi_error: 0.132964\n",
      "[140]\tvalid_0's multi_error: 0.135734\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's multi_error: 0.130194\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.0969529\n",
      "[10]\tvalid_0's multi_error: 0.0803324\n",
      "[15]\tvalid_0's multi_error: 0.0747922\n",
      "[20]\tvalid_0's multi_error: 0.0747922\n",
      "[25]\tvalid_0's multi_error: 0.0637119\n",
      "[30]\tvalid_0's multi_error: 0.0609418\n",
      "[35]\tvalid_0's multi_error: 0.0692521\n",
      "[40]\tvalid_0's multi_error: 0.0692521\n",
      "[45]\tvalid_0's multi_error: 0.066482\n",
      "[50]\tvalid_0's multi_error: 0.0692521\n",
      "[55]\tvalid_0's multi_error: 0.0692521\n",
      "[60]\tvalid_0's multi_error: 0.0692521\n",
      "[65]\tvalid_0's multi_error: 0.0692521\n",
      "[70]\tvalid_0's multi_error: 0.0692521\n",
      "[75]\tvalid_0's multi_error: 0.0692521\n",
      "[80]\tvalid_0's multi_error: 0.0692521\n",
      "[85]\tvalid_0's multi_error: 0.0692521\n",
      "[90]\tvalid_0's multi_error: 0.0692521\n",
      "[95]\tvalid_0's multi_error: 0.0692521\n",
      "[100]\tvalid_0's multi_error: 0.0692521\n",
      "[105]\tvalid_0's multi_error: 0.0692521\n",
      "[110]\tvalid_0's multi_error: 0.0692521\n",
      "[115]\tvalid_0's multi_error: 0.0692521\n",
      "[120]\tvalid_0's multi_error: 0.0692521\n",
      "[125]\tvalid_0's multi_error: 0.0692521\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's multi_error: 0.0581717\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.160665\n",
      "[10]\tvalid_0's multi_error: 0.149584\n",
      "[15]\tvalid_0's multi_error: 0.166205\n",
      "[20]\tvalid_0's multi_error: 0.166205\n",
      "[25]\tvalid_0's multi_error: 0.163435\n",
      "[30]\tvalid_0's multi_error: 0.174515\n",
      "[35]\tvalid_0's multi_error: 0.180055\n",
      "[40]\tvalid_0's multi_error: 0.188366\n",
      "[45]\tvalid_0's multi_error: 0.185596\n",
      "[50]\tvalid_0's multi_error: 0.180055\n",
      "[55]\tvalid_0's multi_error: 0.182825\n",
      "[60]\tvalid_0's multi_error: 0.182825\n",
      "[65]\tvalid_0's multi_error: 0.182825\n",
      "[70]\tvalid_0's multi_error: 0.182825\n",
      "[75]\tvalid_0's multi_error: 0.182825\n",
      "[80]\tvalid_0's multi_error: 0.182825\n",
      "[85]\tvalid_0's multi_error: 0.182825\n",
      "[90]\tvalid_0's multi_error: 0.182825\n",
      "[95]\tvalid_0's multi_error: 0.180055\n",
      "[100]\tvalid_0's multi_error: 0.180055\n",
      "[105]\tvalid_0's multi_error: 0.180055\n",
      "[110]\tvalid_0's multi_error: 0.180055\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's multi_error: 0.144044\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.124654\n",
      "[10]\tvalid_0's multi_error: 0.113573\n",
      "[15]\tvalid_0's multi_error: 0.105263\n",
      "[20]\tvalid_0's multi_error: 0.105263\n",
      "[25]\tvalid_0's multi_error: 0.102493\n",
      "[30]\tvalid_0's multi_error: 0.0969529\n",
      "[35]\tvalid_0's multi_error: 0.102493\n",
      "[40]\tvalid_0's multi_error: 0.0969529\n",
      "[45]\tvalid_0's multi_error: 0.0941828\n",
      "[50]\tvalid_0's multi_error: 0.0914127\n",
      "[55]\tvalid_0's multi_error: 0.0914127\n",
      "[60]\tvalid_0's multi_error: 0.0914127\n",
      "[65]\tvalid_0's multi_error: 0.0914127\n",
      "[70]\tvalid_0's multi_error: 0.0914127\n",
      "[75]\tvalid_0's multi_error: 0.0914127\n",
      "[80]\tvalid_0's multi_error: 0.0914127\n",
      "[85]\tvalid_0's multi_error: 0.0914127\n",
      "[90]\tvalid_0's multi_error: 0.0914127\n",
      "[95]\tvalid_0's multi_error: 0.0914127\n",
      "[100]\tvalid_0's multi_error: 0.0914127\n",
      "[105]\tvalid_0's multi_error: 0.0914127\n",
      "[110]\tvalid_0's multi_error: 0.0914127\n",
      "[115]\tvalid_0's multi_error: 0.0914127\n",
      "[120]\tvalid_0's multi_error: 0.0914127\n",
      "[125]\tvalid_0's multi_error: 0.0914127\n",
      "[130]\tvalid_0's multi_error: 0.0914127\n",
      "[135]\tvalid_0's multi_error: 0.0914127\n",
      "[140]\tvalid_0's multi_error: 0.0914127\n",
      "[145]\tvalid_0's multi_error: 0.0914127\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's multi_error: 0.0914127\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.191136\n",
      "[10]\tvalid_0's multi_error: 0.180055\n",
      "[15]\tvalid_0's multi_error: 0.196676\n",
      "[20]\tvalid_0's multi_error: 0.207756\n",
      "[25]\tvalid_0's multi_error: 0.207756\n",
      "[30]\tvalid_0's multi_error: 0.204986\n",
      "[35]\tvalid_0's multi_error: 0.202216\n",
      "[40]\tvalid_0's multi_error: 0.199446\n",
      "[45]\tvalid_0's multi_error: 0.196676\n",
      "[50]\tvalid_0's multi_error: 0.196676\n",
      "[55]\tvalid_0's multi_error: 0.193906\n",
      "[60]\tvalid_0's multi_error: 0.191136\n",
      "[65]\tvalid_0's multi_error: 0.191136\n",
      "[70]\tvalid_0's multi_error: 0.193906\n",
      "[75]\tvalid_0's multi_error: 0.193906\n",
      "[80]\tvalid_0's multi_error: 0.193906\n",
      "[85]\tvalid_0's multi_error: 0.193906\n",
      "[90]\tvalid_0's multi_error: 0.193906\n",
      "[95]\tvalid_0's multi_error: 0.193906\n",
      "[100]\tvalid_0's multi_error: 0.193906\n",
      "[105]\tvalid_0's multi_error: 0.193906\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's multi_error: 0.174515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.130194\n",
      "[10]\tvalid_0's multi_error: 0.108033\n",
      "[15]\tvalid_0's multi_error: 0.110803\n",
      "[20]\tvalid_0's multi_error: 0.110803\n",
      "[25]\tvalid_0's multi_error: 0.110803\n",
      "[30]\tvalid_0's multi_error: 0.121884\n",
      "[35]\tvalid_0's multi_error: 0.113573\n",
      "[40]\tvalid_0's multi_error: 0.130194\n",
      "[45]\tvalid_0's multi_error: 0.124654\n",
      "[50]\tvalid_0's multi_error: 0.124654\n",
      "[55]\tvalid_0's multi_error: 0.124654\n",
      "[60]\tvalid_0's multi_error: 0.127424\n",
      "[65]\tvalid_0's multi_error: 0.127424\n",
      "[70]\tvalid_0's multi_error: 0.127424\n",
      "[75]\tvalid_0's multi_error: 0.124654\n",
      "[80]\tvalid_0's multi_error: 0.124654\n",
      "[85]\tvalid_0's multi_error: 0.124654\n",
      "[90]\tvalid_0's multi_error: 0.124654\n",
      "[95]\tvalid_0's multi_error: 0.124654\n",
      "[100]\tvalid_0's multi_error: 0.124654\n",
      "[105]\tvalid_0's multi_error: 0.121884\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's multi_error: 0.099723\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.130194\n",
      "[10]\tvalid_0's multi_error: 0.121884\n",
      "[15]\tvalid_0's multi_error: 0.121884\n",
      "[20]\tvalid_0's multi_error: 0.102493\n",
      "[25]\tvalid_0's multi_error: 0.0969529\n",
      "[30]\tvalid_0's multi_error: 0.105263\n",
      "[35]\tvalid_0's multi_error: 0.0969529\n",
      "[40]\tvalid_0's multi_error: 0.102493\n",
      "[45]\tvalid_0's multi_error: 0.105263\n",
      "[50]\tvalid_0's multi_error: 0.102493\n",
      "[55]\tvalid_0's multi_error: 0.108033\n",
      "[60]\tvalid_0's multi_error: 0.105263\n",
      "[65]\tvalid_0's multi_error: 0.105263\n",
      "[70]\tvalid_0's multi_error: 0.105263\n",
      "[75]\tvalid_0's multi_error: 0.105263\n",
      "[80]\tvalid_0's multi_error: 0.105263\n",
      "[85]\tvalid_0's multi_error: 0.105263\n",
      "[90]\tvalid_0's multi_error: 0.105263\n",
      "[95]\tvalid_0's multi_error: 0.105263\n",
      "[100]\tvalid_0's multi_error: 0.105263\n",
      "[105]\tvalid_0's multi_error: 0.102493\n",
      "[110]\tvalid_0's multi_error: 0.102493\n",
      "[115]\tvalid_0's multi_error: 0.105263\n",
      "[120]\tvalid_0's multi_error: 0.099723\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's multi_error: 0.0914127\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.213296\n",
      "[10]\tvalid_0's multi_error: 0.199446\n",
      "[15]\tvalid_0's multi_error: 0.224377\n",
      "[20]\tvalid_0's multi_error: 0.227147\n",
      "[25]\tvalid_0's multi_error: 0.224377\n",
      "[30]\tvalid_0's multi_error: 0.227147\n",
      "[35]\tvalid_0's multi_error: 0.213296\n",
      "[40]\tvalid_0's multi_error: 0.202216\n",
      "[45]\tvalid_0's multi_error: 0.207756\n",
      "[50]\tvalid_0's multi_error: 0.213296\n",
      "[55]\tvalid_0's multi_error: 0.213296\n",
      "[60]\tvalid_0's multi_error: 0.221607\n",
      "[65]\tvalid_0's multi_error: 0.221607\n",
      "[70]\tvalid_0's multi_error: 0.218837\n",
      "[75]\tvalid_0's multi_error: 0.216066\n",
      "[80]\tvalid_0's multi_error: 0.221607\n",
      "[85]\tvalid_0's multi_error: 0.221607\n",
      "[90]\tvalid_0's multi_error: 0.218837\n",
      "[95]\tvalid_0's multi_error: 0.213296\n",
      "[100]\tvalid_0's multi_error: 0.213296\n",
      "[105]\tvalid_0's multi_error: 0.213296\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's multi_error: 0.196676\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.290859\n",
      "[10]\tvalid_0's multi_error: 0.282548\n",
      "[15]\tvalid_0's multi_error: 0.279778\n",
      "[20]\tvalid_0's multi_error: 0.288089\n",
      "[25]\tvalid_0's multi_error: 0.288089\n",
      "[30]\tvalid_0's multi_error: 0.288089\n",
      "[35]\tvalid_0's multi_error: 0.285319\n",
      "[40]\tvalid_0's multi_error: 0.288089\n",
      "[45]\tvalid_0's multi_error: 0.299169\n",
      "[50]\tvalid_0's multi_error: 0.293629\n",
      "[55]\tvalid_0's multi_error: 0.290859\n",
      "[60]\tvalid_0's multi_error: 0.282548\n",
      "[65]\tvalid_0's multi_error: 0.282548\n",
      "[70]\tvalid_0's multi_error: 0.282548\n",
      "[75]\tvalid_0's multi_error: 0.282548\n",
      "[80]\tvalid_0's multi_error: 0.277008\n",
      "[85]\tvalid_0's multi_error: 0.277008\n",
      "[90]\tvalid_0's multi_error: 0.277008\n",
      "[95]\tvalid_0's multi_error: 0.279778\n",
      "[100]\tvalid_0's multi_error: 0.279778\n",
      "[105]\tvalid_0's multi_error: 0.277008\n",
      "[110]\tvalid_0's multi_error: 0.277008\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's multi_error: 0.274238\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.171745\n",
      "[10]\tvalid_0's multi_error: 0.174515\n",
      "[15]\tvalid_0's multi_error: 0.168975\n",
      "[20]\tvalid_0's multi_error: 0.177285\n",
      "[25]\tvalid_0's multi_error: 0.166205\n",
      "[30]\tvalid_0's multi_error: 0.171745\n",
      "[35]\tvalid_0's multi_error: 0.168975\n",
      "[40]\tvalid_0's multi_error: 0.157895\n",
      "[45]\tvalid_0's multi_error: 0.155125\n",
      "[50]\tvalid_0's multi_error: 0.152355\n",
      "[55]\tvalid_0's multi_error: 0.155125\n",
      "[60]\tvalid_0's multi_error: 0.149584\n",
      "[65]\tvalid_0's multi_error: 0.146814\n",
      "[70]\tvalid_0's multi_error: 0.149584\n",
      "[75]\tvalid_0's multi_error: 0.149584\n",
      "[80]\tvalid_0's multi_error: 0.152355\n",
      "[85]\tvalid_0's multi_error: 0.152355\n",
      "[90]\tvalid_0's multi_error: 0.155125\n",
      "[95]\tvalid_0's multi_error: 0.155125\n",
      "[100]\tvalid_0's multi_error: 0.155125\n",
      "[105]\tvalid_0's multi_error: 0.157895\n",
      "[110]\tvalid_0's multi_error: 0.157895\n",
      "[115]\tvalid_0's multi_error: 0.157895\n",
      "[120]\tvalid_0's multi_error: 0.157895\n",
      "[125]\tvalid_0's multi_error: 0.157895\n",
      "[130]\tvalid_0's multi_error: 0.157895\n",
      "[135]\tvalid_0's multi_error: 0.157895\n",
      "[140]\tvalid_0's multi_error: 0.157895\n",
      "[145]\tvalid_0's multi_error: 0.157895\n",
      "[150]\tvalid_0's multi_error: 0.157895\n",
      "[155]\tvalid_0's multi_error: 0.157895\n",
      "[160]\tvalid_0's multi_error: 0.157895\n",
      "[165]\tvalid_0's multi_error: 0.157895\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's multi_error: 0.146814\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.238227\n",
      "[10]\tvalid_0's multi_error: 0.232687\n",
      "[15]\tvalid_0's multi_error: 0.252078\n",
      "[20]\tvalid_0's multi_error: 0.260388\n",
      "[25]\tvalid_0's multi_error: 0.271468\n",
      "[30]\tvalid_0's multi_error: 0.271468\n",
      "[35]\tvalid_0's multi_error: 0.265928\n",
      "[40]\tvalid_0's multi_error: 0.265928\n",
      "[45]\tvalid_0's multi_error: 0.268698\n",
      "[50]\tvalid_0's multi_error: 0.265928\n",
      "[55]\tvalid_0's multi_error: 0.265928\n",
      "[60]\tvalid_0's multi_error: 0.265928\n",
      "[65]\tvalid_0's multi_error: 0.265928\n",
      "[70]\tvalid_0's multi_error: 0.263158\n",
      "[75]\tvalid_0's multi_error: 0.263158\n",
      "[80]\tvalid_0's multi_error: 0.263158\n",
      "[85]\tvalid_0's multi_error: 0.263158\n",
      "[90]\tvalid_0's multi_error: 0.263158\n",
      "[95]\tvalid_0's multi_error: 0.263158\n",
      "[100]\tvalid_0's multi_error: 0.263158\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's multi_error: 0.202216\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.227147\n",
      "[10]\tvalid_0's multi_error: 0.227147\n",
      "[15]\tvalid_0's multi_error: 0.246537\n",
      "[20]\tvalid_0's multi_error: 0.238227\n",
      "[25]\tvalid_0's multi_error: 0.238227\n",
      "[30]\tvalid_0's multi_error: 0.238227\n",
      "[35]\tvalid_0's multi_error: 0.235457\n",
      "[40]\tvalid_0's multi_error: 0.240997\n",
      "[45]\tvalid_0's multi_error: 0.235457\n",
      "[50]\tvalid_0's multi_error: 0.235457\n",
      "[55]\tvalid_0's multi_error: 0.235457\n",
      "[60]\tvalid_0's multi_error: 0.235457\n",
      "[65]\tvalid_0's multi_error: 0.235457\n",
      "[70]\tvalid_0's multi_error: 0.235457\n",
      "[75]\tvalid_0's multi_error: 0.235457\n",
      "[80]\tvalid_0's multi_error: 0.238227\n",
      "[85]\tvalid_0's multi_error: 0.238227\n",
      "[90]\tvalid_0's multi_error: 0.238227\n",
      "[95]\tvalid_0's multi_error: 0.240997\n",
      "[100]\tvalid_0's multi_error: 0.240997\n",
      "[105]\tvalid_0's multi_error: 0.240997\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's multi_error: 0.218837\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.199446\n",
      "[10]\tvalid_0's multi_error: 0.199446\n",
      "[15]\tvalid_0's multi_error: 0.204986\n",
      "[20]\tvalid_0's multi_error: 0.207756\n",
      "[25]\tvalid_0's multi_error: 0.204986\n",
      "[30]\tvalid_0's multi_error: 0.207756\n",
      "[35]\tvalid_0's multi_error: 0.204986\n",
      "[40]\tvalid_0's multi_error: 0.202216\n",
      "[45]\tvalid_0's multi_error: 0.204986\n",
      "[50]\tvalid_0's multi_error: 0.199446\n",
      "[55]\tvalid_0's multi_error: 0.202216\n",
      "[60]\tvalid_0's multi_error: 0.213296\n",
      "[65]\tvalid_0's multi_error: 0.207756\n",
      "[70]\tvalid_0's multi_error: 0.202216\n",
      "[75]\tvalid_0's multi_error: 0.204986\n",
      "[80]\tvalid_0's multi_error: 0.207756\n",
      "[85]\tvalid_0's multi_error: 0.204986\n",
      "[90]\tvalid_0's multi_error: 0.207756\n",
      "[95]\tvalid_0's multi_error: 0.204986\n",
      "[100]\tvalid_0's multi_error: 0.204986\n",
      "[105]\tvalid_0's multi_error: 0.204986\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's multi_error: 0.196676\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns=['train_who', 'train_porder', 'train_score', 'valid_who', 'valid_porder', 'valid_score', 'test_who', 'test_porder', 'test_score', 'multi_error'])\n",
    "id_ = 0\n",
    "\n",
    "for train_who in range(1, 8 + 1):\n",
    "    for train_porder in range(1, 3 + 1):\n",
    "        for valid_porder in range(1, 3 + 1):\n",
    "            if train_porder == valid_porder:\n",
    "                continue\n",
    "            x, y, x_valid, y_valid = prepare(train_who, train_porder, valid_porder)\n",
    "            \n",
    "            train_set = lgb.Dataset(x, y)\n",
    "            valid_set = lgb.Dataset(x_valid, y_valid, free_raw_data=False)\n",
    "            \n",
    "            watchlist = [valid_set]\n",
    "            \n",
    "            model = lgb.train(params, train_set=train_set, valid_sets=watchlist, num_boost_round=200, verbose_eval=5, early_stopping_rounds=100)\n",
    "            \n",
    "            for test_who in range(1, 8 + 1):\n",
    "                for test_porder in range(1, 3 + 1):\n",
    "                    if test_who == train_who and (test_porder == train_porder or test_porder == valid_porder):\n",
    "                        continue\n",
    "                \n",
    "                    x_test = pfs[test_who - 1][test_porder - 1].event_primitive_df.drop(['hit_type'], axis=1)\n",
    "                    y_true = pfs[test_who - 1][test_porder - 1].event_primitive_df['hit_type']\n",
    "                    \n",
    "                    y_pred = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "                    predictions = []\n",
    "                    for xx in y_pred:\n",
    "                        predictions.append(np.argmax(xx))\n",
    "                    y_test = pd.Series(data=predictions)\n",
    "                    \n",
    "                    multierror = '%.4f' % get_multi_error(y_true, y_test)\n",
    "                    \n",
    "#                     train_str = '%d,%d' % (train_who, train_porder)\n",
    "                    train_score = sfs[train_who - 1][train_porder - 1]\n",
    "                    \n",
    "#                     valid_str = '%d,%d' % (train_who, valid_porder)\n",
    "                    valid_score = sfs[train_who - 1][valid_porder - 1]\n",
    "                                                   \n",
    "#                     test_str = '%d,%d' % (test_who, test_porder)\n",
    "                    test_score = sfs[test_who - 1][test_porder - 1]\n",
    "                    \n",
    "                    result_df.loc[id_] = [train_who, train_porder, train_score, train_who, valid_porder, valid_score, test_who, test_porder, test_score, multierror]\n",
    "                    \n",
    "                    id_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T07:55:21.024889Z",
     "start_time": "2018-07-26T07:55:21.011780Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df.to_csv('song1_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
